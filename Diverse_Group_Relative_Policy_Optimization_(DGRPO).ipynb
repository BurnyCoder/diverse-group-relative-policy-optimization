{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diverse Group Relative Policy Optimization (DGRPO)"
      ],
      "metadata": {
        "id": "EwzEpePq83Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "GRPO explain with code and training example is [An overview of GRPO & DeepSeek-R1 Training with Open Source GRPO Model Fine Tuning](https://github.com/ALucek/GRPO-Training/tree/main) by Adam Lucek"
      ],
      "metadata": {
        "id": "zi-6baFG8-C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Relative Policy Optimization (GRPO)\n",
        "\n",
        "<img src=\"https://i0.wp.com/chatgptfrancais.org/wp-content/uploads/2025/02/open-ai-o3-mini.jpg?resize=900%2C506&ssl=1\" width=600>\n",
        "\n",
        "DeepSeek-R1 has disrupted the AI community by providing a competitive, comparable, and completely open sourced alternative to OpenAI's original reasoning models. Before the DeepSeek team released their report, the methods and techniques used to enable long reflective **reasoning** capabilities in language models have been largely speculative.\n",
        "\n",
        "While it's still not clear exactly how the private labs like OpenAI have achieved their own reasoning, DeepSeek fully outlines their technique in the paper [*DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
        "Reinforcement Learning*](https://arxiv.org/pdf/2501.12948).\n",
        "\n",
        "Within the paper they outline the entire training pipeline for [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) along with their breakthrough using a new reinforcement learning technique, Group Relative Policy Optimization (GRPO), originally outlined in [*DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*](https://arxiv.org/pdf/2402.03300).\n",
        "\n",
        "In this notebook we'll be show how GRPO is applied by covering:\n",
        "1. How the Algorithm Works\n",
        "2. The DeepSeek-R1 Training Pipeline\n",
        "3. Applying GRPO Training Ourselves to Qwen-2.5-3B-Instruct"
      ],
      "metadata": {
        "id": "PLujFAt751QU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# How GRPO Works\n",
        "\n",
        "<img src=\"https://community.aws/_next/image?url=https%3A%2F%2Fassets.community.aws%2Fa%2F2ra6RfOUylhUM8M3eDEMfDlB3l7%2FScre.webp%3FimgSize%3D1698x894&w=3840&q=75\" width=600>\n",
        "\n",
        "Group Relative Policy Optimization is a modified version of Proximal Policy Optimization (PPO), a popular reinforcement learning algorithm commonly used for techniques like Reinforcement Learning from Human Feedback (RLHF).\n",
        "\n",
        "In PPO for LLM alignment, the process tends to follow:\n",
        "1. Sample response from the policy model (i.e., the LLM being fine-tuned).\n",
        "2. Compute total reward from a reward function(s)/model(s) (typically trained on human preference data or aligned to LLM behavior).\n",
        "3. Compute KL (Kullback-Leibler) penalty using a reference model (often the original frozen model or a lagged training model) to ensure the policy update does not deviate too much.\n",
        "4. Combine reward and KL penalty to compute the final reward used for training.\n",
        "5. Use a separate value model (often the same LLM with an extra value head, or seperately trained value model) to estimate the value function (i.e., how good the current state-action pair is).\n",
        "6. Compute the advantage function using Generalized Advantage Estimation (GAE) to determine how much better an action is compared to the expected value of the current state.\n",
        "7. Update the policy using the PPO objective function.\n",
        "\n",
        "GRPO modifies this by taking multiple samples for each **input prompt**, then computing rewards **relatively within each batch** rather than assigning absolute values.  \n",
        "\n",
        "<img src=\"https://www.philschmid.de/static/blog/deepseek-r1/grpo.png\" width=600>\n",
        "\n",
        "<img src=\"https://i.imgur.com/xjMWlkC.png\" width=600>\n",
        "\n",
        "In GRPO, the process follows:  \n",
        "1. Sample multiple responses from the policy model for the same input prompt.  \n",
        "2. Compute rewards for each response using a reward function(s)/model(s).  \n",
        "3. Normalize rewards within the group by computing the mean and standard deviation.  \n",
        "4. Compute advantage values using the difference between a response’s reward and the group’s mean reward.\n",
        "5. Compute KL penalty using a reference model to prevent excessive divergence.  \n",
        "6. Use the GRPO objective function to update the policy, now optimizing for relative ranking instead of absolute values. This means GRPO cares about how well a response did compared to other responses for the same prompt, rather than trying to learn absolute reward values.\n",
        "\n",
        "By **removing the need for a separate value model** and focusing on **group-relative scoring**, GRPO makes training more efficient while maintaining alignment with human feedback and training examples."
      ],
      "metadata": {
        "id": "EafYm4slPbnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# How GRPO Was Applied\n",
        "\n",
        "GRPO turned out to be a powerful algorithm for teaching LLMs to think through problems longer without explicitly defining any functions that encourage reasoning or thinking length.\n",
        "\n",
        "DeepSeek originally began training their model purely using GRPO reinforcement learning on R1's predecessor, [DeepSeek-R1-Zero](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero). The setup for this included:\n",
        "\n",
        "* Starting with the [DeepkSeek-V3 LLM](https://arxiv.org/pdf/2412.19437v1) as the base model\n",
        "* Using the below prompt template:\n",
        "\n",
        "<img src=\"https://i.postimg.cc/0yk4fgw3/Screenshot-2025-02-16-at-10-42-37-AM.png\" width=600>\n",
        "\n",
        "* Defining reward functions for **formatting** and **accuracy**\n",
        "\n",
        "Notably, this initial approach does not rely on any supervised fine tuning (SFT) to try and explicitly train the model on reasoning examples or processes, rather relying on the RL environment to guide the LLM to learn this behavior itself. Additionally, the training dataset primarily consisted of examples with verifiable outcomes like math/code/logic based questions, as these outputs are much easier to verify the accuracy of for reward modeling.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*DJUCCE_aN14OpSy4\" width=600>\n",
        "\n",
        "The result of this training was successful, with a few surprising observations as the model began to:\n",
        "1. Exhibit the behavior of reflection- revisiting and reevaluating prior steps in it's thought process.\n",
        "2. Consider and explore possible alternative methods for solving problems rather than sticking to a single or the first option.\n",
        "3. Use more test-time compute and generate longer answers as it reflects and explores more possibilities, as shown above.\n",
        "\n",
        "This proved DeepSeek's theory of being able to use purely RL to guide LLMs towards learning advanced reasoning capabilities, leading to the now famous *aha moment* observed during intermediate training runs.\n",
        "\n",
        "<img src=\"https://bgr.com/wp-content/uploads/2025/01/deepseek-r1-aha-moment.jpg?quality=82&strip=all\" width=600>\n",
        "\n",
        "With the initial success of DeepSeek-R1-Zero, the team moved on to applying this in a more formalized LLM training and alignment pipeline.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*aVbpOkbtbSznComBWkijsQ.png\" width=600>\n",
        "\n",
        "The training of DeepSeek-R1 follows a 4 step process:\n",
        "1. **SFT with Long CoT Examples**: Fine tune DeepSeek-V3 with thousands of labeled reasoning examples gathered from DeepSeek-Zero, Human Annotators, and Few Shot synthetic data generation techniques.\n",
        "2. **GRPO RL for Reasoning**: An initial training run using GRPO on reasoning-oriented examples (coding, math, science, logic) to enhance the model's reasoning capabilities.\n",
        "3. **Rejection Sampling & SFT**: Post stage 2, the resulting checkpoint was used along with DeepSeek-V3 as a judge and V3's original training data to generate and augment additional training examples. This now includes non-reasoning specific data like writing, factual QA, and translation, resulting in 800k curated examples for general-purpose tasks. The model was then trained using SFT for 2 epochs on these examples.\n",
        "4. **RL Alignment**: Finally, GRPO RL was employed again for the final harmlessness and helpfulness alignment. A combination of reasoning specific rewards and human preference model rewards, along with a final diverse mix of training data is used to create the end result DeepSeek-R1 model.\n",
        "\n",
        "GRPO proved to be a powerful foundation for training reasoning capabilities LLMs as shown in DeepSeek-R1's development. Through group-relative scoring and removing the need for a value model, the team needed only rule-based rewards for accuracy and format validation to get initial reasoning results. Adding supervised fine-tuning before GRPO training made the process more stable and efficient compared to R1-Zero's pure RL approach. Their result matched OpenAI's o1-1217 performance but with significantly reduced compute requirements forgoing extra critic models and complex reward models. Most notably, GRPO's relative optimization enabled strong generalization of reasoning patterns from technical domains to broader tasks, suggesting that group-relative training may be particularly effective at teaching foundational reasoning capabilities in LLMs."
      ],
      "metadata": {
        "id": "1P06D0StdV60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Applying GRPO Ourselves\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png\" width=600>\n",
        "\n",
        "Now that we have an understanding of GRPO and the way it's been applied to create the (current) best open source reasoning model- let's apply some of the techniques ourselves.\n",
        "\n",
        "While we won't be [applying the full pipeline](https://github.com/huggingface/open-r1) in this example, we can take a base language model and use GRPO reinforcement learning to start training reasoning capabilities.\n",
        "\n",
        "Below we will outline the process of training [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) on [GSM8K](https://huggingface.co/datasets/openai/gsm8k) math problems with GRPO and how this affects its' chain-of-thought reasoning ability.\n",
        "\n",
        "The below code is a modified and annotated version of [Unsloth's original](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl) documentation and code. Shoutout to the Han brothers for their incredible work! Click the link for more information and examples from Unsloth."
      ],
      "metadata": {
        "id": "vBLlBF-5feU7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1gpClh1hqLe"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We will be relying on [Unsloth](https://github.com/unslothai/unsloth), [vLLM](https://github.com/vllm-project/vllm) and [Transformers Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) as the core packages for training and inferencing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ZO3gvF-hqLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2433c0-38ee-41a0-f937-ee9dbf7af314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: unsloth 2025.3.18\n",
            "Uninstalling unsloth-2025.3.18:\n",
            "  Successfully uninstalled unsloth-2025.3.18\n",
            "Collecting unsloth\n",
            "  Using cached unsloth-2025.3.18-py3-none-any.whl.metadata (46 kB)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.3.14 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.3.16)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.3)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.17)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.49.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.4.1)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.0.dev0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.29.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Using cached unsloth-2025.3.18-py3-none-any.whl (192 kB)\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2025.3.18\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.49.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (3.20.3)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.11)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.14)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.66.3)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.10.6)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.11)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.16 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.16)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.12.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (1.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.9.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.2)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.14.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.60.0)\n",
            "Requirement already satisfied: ray>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]>=2.43.0->vllm) (2.44.0)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.29.post2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.60.0->vllm) (0.43.0)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20250224)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.1)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (24.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (4.11.0.86)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (1.5.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.19.1->vllm) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.48.2->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.18.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.13.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.23.1)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
            "  Cloning https://github.com/huggingface/trl.git (to revision e95f9fb74a3c3647b86f251b7e230ec51c64b72b) to /tmp/pip-req-build-vk8bjupo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-vk8bjupo\n",
            "  Running command git rev-parse -q --verify 'sha^e95f9fb74a3c3647b86f251b7e230ec51c64b72b'\n",
            "  Running command git fetch -q https://github.com/huggingface/trl.git e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
            "  Running command git checkout -q e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
            "  Resolved https://github.com/huggingface/trl.git to commit e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.15.0.dev0) (1.5.2)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.15.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.15.0.dev0) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.15.0.dev0) (4.49.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl==0.15.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.0.dev0) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl==0.15.0.dev0) (3.11.14)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl==0.15.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl==0.15.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.15.0.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.15.0.dev0) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.15.0.dev0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl==0.15.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.15.0.dev0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.15.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.15.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.15.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.15.0.dev0) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl==0.15.0.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl==0.15.0.dev0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl==0.15.0.dev0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl==0.15.0.dev0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl==0.15.0.dev0) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "!pip uninstall unsloth -y\n",
        "!pip install unsloth --no-binary numpy\n",
        "!pip install vllm\n",
        "!pip install --upgrade pillow\n",
        "# If you are running this notebook locally (not colab), you need to install `diffusers` too\n",
        "# !pip install diffusers\n",
        "\n",
        "# Temporarily install a specific TRL nightly version that supports GRPO\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDvdAzWbhqLg"
      },
      "source": [
        "## Unsloth TRL Patch\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:898/0*tporAgJg5LLllQe_.png\" width=250>\n",
        "\n",
        "Unsloth's `PatchFastRL` modifies TRL (Transformer Reinforcement Learning) trainers to work efficiently with Unsloth's optimized models. The patch makes three key changes:\n",
        "\n",
        "1. Patches the model unwrapping process for efficient generation during training\n",
        "2. Modifies TRL trainers with Unsloth-specific optimizations, including things like:\n",
        "   - Automatically handles mixed precision training (FP16/BF16) based on model config\n",
        "   - Sets optimal defaults for hyperparameters like learning rate, batch sizes, and gradient accumulation\n",
        "   - Adds safety checks for learning rates and sequence lengths\n",
        "   - Configures tokenizer padding and model inference settings\n",
        "   - Integrates with vLLM for faster inference when available\n",
        "   - And more!\n",
        "3. Sets up algorithm-specific statistics tracking (in our case, for GRPO)\n",
        "\n",
        "You can check out the implementation details [in their code here](https://github.com/unslothai/unsloth/blob/d1d15f1d14f1168837d29b9c08e9b6d63945d469/unsloth/models/rl.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DIs5BMcvjN",
        "outputId": "2a5ead6f-1512-410f-e29a-8b8d85ef7b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 03-23 23:24:54 [__init__.py:256] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "\n",
        "# Execute the Patch\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joje4qPsyxM9"
      },
      "source": [
        "## Load Base Model\n",
        "\n",
        "<img src=\"https://the-decoder.com/wp-content/uploads/2024/09/Qwen2.5-title.png\" width=300>\n",
        "\n",
        "For our base model, we'll be using [Qwen 2.5 3B Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct).\n",
        "\n",
        "To save resources and time, we'll be using parameter efficient fine-tuning (PEFT) methods, such as quantized training and LoRA adapters for our GRPO training, and taking advantage of Unsloth's optimized LLM [unsloth/Qwen2.5-3B-Instruct-bnb-4bit](https://huggingface.co/unsloth/Qwen2.5-3B-Instruct-bnb-4bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DkIvEkIIkEyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656,
          "referenced_widgets": [
            "32faecb3daa949949bef53c6e5a62ee0",
            "ee26cff63af747af934281ac5ffcbc85",
            "0ac2cd5c7e6f49ddac2bb94906f5ca34",
            "0f11f0e1beea415a8ca17c8108eb2112",
            "63c3c69b9e534b48b746b0b7f72ccaea",
            "9b885df2b0994d0581375aa5a9f050d5",
            "f23da13a375242379efff25fac78c164",
            "000eef1efc51416b83c4f0a2adbe703d",
            "9035c8de1e194d50bceeead68cd89708",
            "628a653b5e274cb2938de68e50c5e102",
            "06990b32820e4975a6529f10e0c35255",
            "cbe5bf88dca94691ab8c8457c9c986d3",
            "46eae2205f5b427cbd23fdeb702d5d7b",
            "9d37116b2b5b4d53b0a0b700d685783c",
            "609ced53e08e4b51b1d24dffa1b08e69",
            "132779d2b05b42c1948d6485aed3be84",
            "561acbb114cb4991845f04e5b2dd3e94",
            "55851eeb01e745bbbf0113ce2e489361",
            "b59728bb84c045e2b48e9286b078e598",
            "dd10ee1bfe66483abda594687e5998f9",
            "ae719b74a7aa49e5814fa0e77aee1edc",
            "df040e33f55d4a1e907f80405be8b799"
          ]
        },
        "outputId": "2477b2b8-122a-4c09-deed-be90cddde2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.8.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 39.54%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 256.\n",
            "Unsloth: vLLM's KV Cache can use up to 13.22 GB. Also swap space = 6 GB.\n",
            "INFO 03-23 23:25:12 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
            "WARNING 03-23 23:25:12 [arg_utils.py:1765] --quantization bitsandbytes is not supported by the V1 Engine. Falling back to V0. \n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 03-23 23:25:12 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 03-23 23:25:13 [cuda.py:285] Using Flash Attention backend.\n",
            "INFO 03-23 23:25:14 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 03-23 23:25:14 [model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 03-23 23:25:14 [loader.py:1137] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 03-23 23:25:16 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
            "INFO 03-23 23:25:16 [weight_utils.py:307] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32faecb3daa949949bef53c6e5a62ee0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbe5bf88dca94691ab8c8457c9c986d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-23 23:25:18 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 03-23 23:25:18 [model_runner.py:1146] Model loading took 2.4392 GB and 4.121201 seconds\n",
            "INFO 03-23 23:25:22 [worker.py:267] Memory profiling takes 2.84 seconds\n",
            "INFO 03-23 23:25:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.40) = 15.64GiB\n",
            "INFO 03-23 23:25:22 [worker.py:267] model weights take 2.44GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 11.70GiB.\n",
            "INFO 03-23 23:25:22 [executor_base.py:111] # cuda blocks: 21301, # CPU blocks: 10922\n",
            "INFO 03-23 23:25:22 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 166.41x\n",
            "INFO 03-23 23:25:27 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:49<00:00,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-23 23:26:17 [model_runner.py:1570] Graph capturing finished in 50 secs, took 0.72 GiB\n",
            "INFO 03-23 23:26:17 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 58.24 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Unsloth 2025.3.18 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load Base Model & Tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = 2048,                      # Can increase for longer reasoning traces\n",
        "    load_in_4bit = True,                        # False for LoRA 16bit\n",
        "    fast_inference = True,                      # Enable vLLM fast inference\n",
        "    max_lora_rank = 64,                         # Larger rank = smarter, but slower\n",
        "    gpu_memory_utilization = 0.4,               # Reduce if out of memory\n",
        ")\n",
        "\n",
        "# Prepare Model for Parameter Efficient Fine Tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,                                     # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 64,                            # LoRA Rank\n",
        "    use_gradient_checkpointing = \"unsloth\",     # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y56ln_izS9E"
      },
      "source": [
        "### Dataset Preparation\n",
        "\n",
        "*The below data prep and reward functions come from [@willccbb's work](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb), check out his original implementation for more!*\n",
        "\n",
        "<img src=\"https://i.postimg.cc/QC42P5y2/Screenshot-2025-02-16-at-2-35-20-PM.png\" width=600>\n",
        "\n",
        "As shown by DeepSeek's training, it's best to start teaching reasoning capabiltiies with examples that are easily verifiable. In that case, we'll be using math problem examples from [GSM8K](https://huggingface.co/datasets/openai/gsm8k), a benchmark from OpenAI of grade school math word problems. These are perfect as we have straight forward problems and can clearly check the solution.\n",
        "\n",
        "Similar to what was done in DeepSeek-R1-Zero's original training, we'll provide a prompt template to encourage (but not teach!) the model to reason:\n",
        "\n",
        "```\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "```\n",
        "\n",
        "We take the original examples and format them into the messages that the LLM will be expecting, along with the answers to the problems to help verify."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# Helper Function for Parsing Answer\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()"
      ],
      "metadata": {
        "id": "-b-KdEMRLXkn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Functions\n",
        "\n",
        "<img src=\"https://www.altexsoft.com/static/blog-post/2023/11/345fadfa-549a-462a-b757-9ab258e747f3.jpg\" width=600>\n",
        "\n",
        "As is crucial to reinforcement learning, we must define our reward functions that provide a score based on the model's output. Through training, the LLM will work towards maximizing this score.\n",
        "\n",
        "The most important function will initially be our `correctness_reward_func` as it validates whether the generated answer is right or wrong."
      ],
      "metadata": {
        "id": "Ygh9hJCdPJ-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Function to Extract Answer from LLM Response\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
      ],
      "metadata": {
        "id": "Jjt1yHI5SKIr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`int_reward_func` checks if the answer output is a number. This will encourage the final output to just be the proposed solution and confine the reasoning to the reasoning section."
      ],
      "metadata": {
        "id": "U38QXDLrSev8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`strict_format_reward_func`, `soft_format_reward_func` and `xmlcount_reward_func` enforce that the generated output follows the XML pattern that we expect at a strict, loose, and granular level."
      ],
      "metadata": {
        "id": "0DHjUeuvS4hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "# XML Counting Helper Function\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ],
      "metadata": {
        "id": "nEMB7Y9eS4zS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTnL_tJnzh2L"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_banner_dark.png\" width=400>\n",
        "\n",
        "With our model, dataset, and reward functions ready- we now need to set the hyperparameters for training. As mentioned, we'll be using [TRL's GRPO Support](https://huggingface.co/docs/trl/main/en/grpo_trainer) for training. See link for additional documentation and resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Defining Trainer Arguments\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True,                             # Enable faster inference using vLLM\n",
        "    learning_rate = 5e-6,                        # Small learning rate for stable training\n",
        "    adam_beta1 = 0.9,                           # AdamW optimizer momentum parameter\n",
        "    adam_beta2 = 0.99,                          # AdamW optimizer second moment parameter\n",
        "    weight_decay = 0.1,                         # L2 regularization to prevent overfitting\n",
        "    warmup_ratio = 0.1,                         # Portion of training steps for learning rate warmup\n",
        "    lr_scheduler_type = \"cosine\",               # Learning rate decay schedule type\n",
        "    optim = \"adamw_8bit\",                       # Use 8-bit AdamW optimizer for memory efficiency\n",
        "    logging_steps = 1,                          # Log metrics every step\n",
        "    bf16 = is_bfloat16_supported(),            # Use bfloat16 if hardware supports it\n",
        "    fp16 = not is_bfloat16_supported(),        # Fallback to float16 if bfloat16 not supported\n",
        "    per_device_train_batch_size = 1,           # Number of prompts per GPU\n",
        "    gradient_accumulation_steps = 1,           # Number of steps to accumulate gradients\n",
        "    num_generations = 8,                       # Number of responses to generate per prompt for GRPO\n",
        "    max_prompt_length = 256,                   # Maximum length of input prompts in tokens\n",
        "    max_completion_length = 512,               # Maximum length of model responses in tokens\n",
        "    max_steps = 2000,                         # Total number of training steps\n",
        "    save_steps = 250,                         # Save checkpoint every 250 steps\n",
        "    max_grad_norm = 0.1,                      # Gradient clipping threshold\n",
        "    report_to = \"wandb\",                      # Log metrics to Weights & Biases\n",
        "    output_dir = \"/content/drive/MyDrive/grpo_model/outputs\",  # Directory to save model checkpoints in Google Drive               # Directory to save model checkpoints\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_71Y0eKz5yE"
      },
      "source": [
        "Now we can put the model, tokenizer, reward functions, training arguments, and dataset all together into one `GRPOTrainer`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Trainer\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "LE-qOL56XHGr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally kick off the training!"
      ],
      "metadata": {
        "id": "zh1PfeRVXHs1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "# # Start the Training Run!\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.postimg.cc/3r6MhjRR/Screenshot-2025-02-16-at-3-24-53-PM.png\" width=400>\n",
        "\n",
        "In total, 2000 examples on an L4 with this exact setup took roughly 10 hours to train!\n",
        "\n",
        "<img src=\"https://i.postimg.cc/k5439Tx2/Screenshot-2025-02-16-at-3-28-48-PM.png\" width=600>\n",
        "\n",
        "Check out my full run details on [Weights and Biases here](https://api.wandb.ai/links/adam-lucek/y7ejubyl)!"
      ],
      "metadata": {
        "id": "rLvAU_x7b6bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Model\n",
        "\n",
        "Once our LoRA adapter has been trained, we can merge it with the original model and push to the hub."
      ],
      "metadata": {
        "id": "x_7ZDh08Xjzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mount Google Drive to save models permanently\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Create a directory for our models in Google Drive\n",
        "# import os\n",
        "# model_dir = \"/content/drive/MyDrive/grpo_model\"\n",
        "# lora_dir = \"/content/drive/MyDrive/grpo_model/lora\"\n",
        "# output_dir = \"/content/drive/MyDrive/grpo_model/outputs\"\n",
        "\n",
        "# # Create directories if they don't exist\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "# os.makedirs(lora_dir, exist_ok=True)\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Save the LoRA Adapter to Google Drive\n",
        "# model.save_lora(lora_dir)\n",
        "# print(f\"LoRA adapter saved to: {lora_dir}\")\n",
        "\n",
        "# # Merge to 16bit and save to Google Drive\n",
        "# model.save_pretrained_merged(model_dir, tokenizer, save_method = \"merged_16bit\",)\n",
        "# print(f\"Merged model saved to: {model_dir}\")"
      ],
      "metadata": {
        "id": "dD1qP5bXXNEb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different quantizations, GGUF formatting, and other ways to save or convert your trained model are outlined on the [Unsloth Wiki](https://github.com/unslothai/unsloth/wiki)."
      ],
      "metadata": {
        "id": "9j4eKXu0X56K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Using the Model\n",
        "\n",
        "Now that we have our trained model saved, the below section shows how to load and run the LLM.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/Dw076f8J/Screenshot-2025-02-16-at-3-10-09-PM.png\" width=600>\n",
        "\n",
        "My version trained with this notebook has been pushed to [AdamLucek/Qwen2.5-3B-Instruct-GRPO-2K-GSM8K](https://huggingface.co/AdamLucek/Qwen2.5-3B-Instruct-GRPO-2K-GSM8K)."
      ],
      "metadata": {
        "id": "VyXdcrK6onY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow"
      ],
      "metadata": {
        "id": "kiIxXVLCon7I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from vllm import SamplingParams\n",
        "import torch\n",
        "\n",
        "# # Load the Model & Tokenizer\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"AdamLucek/Qwen2.5-3B-Instruct-GRPO-2K-GSM8K\",\n",
        "#     max_seq_length = 2048,\n",
        "#     load_in_4bit = True,\n",
        "#     fast_inference = True,\n",
        "#     gpu_memory_utilization = 0.4,\n",
        "# )"
      ],
      "metadata": {
        "id": "cdENwH8Potn4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prep the Message\n",
        "\n",
        "PROMPT = \"How many r's are in the word strawberry?\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : PROMPT},\n",
        "], tokenize = False, add_generation_prompt = True)"
      ],
      "metadata": {
        "id": "yLqmMNyTpSNT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate a response\n",
        "# sampling_params = SamplingParams(\n",
        "#     temperature = 0.8,\n",
        "#     top_p = 0.95,\n",
        "#     max_tokens = 1024,\n",
        "# )\n",
        "# output = model.fast_generate(\n",
        "#     text,\n",
        "#     sampling_params = sampling_params,\n",
        "# )[0].outputs[0].text\n",
        "\n",
        "# print(\"\\n\\n\", output)"
      ],
      "metadata": {
        "id": "uNKsO3m9o22d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diverse Group Relative Policy Optimization (DGRPO)"
      ],
      "metadata": {
        "id": "Q_FIdvq6FOH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While GRPO normalizes rewards within groups of responses to promote accuracy, DGRPO incorporates solution diversity into the advantage calculation through two novel approaches: (1) upweighting less likely but correct tokens to incentivize rare solutions, and (2) quantifying solution uniqueness using cosine similarity of neural embeddings."
      ],
      "metadata": {
        "id": "qq5Td0hAFRP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_probabilities(response, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Calculate token probabilities for a given response.\n",
        "\n",
        "    Args:\n",
        "        response: The model's text response\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "\n",
        "    Returns:\n",
        "        Average token probability\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(response, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    # Get token probabilities by taking softmax over logits\n",
        "    token_probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Calculate probability of each generated token\n",
        "    input_ids = inputs.input_ids\n",
        "    probabilities = []\n",
        "    for i in range(1, len(input_ids[0])):\n",
        "        token_id = input_ids[0][i].item()\n",
        "        # Get the probability at position i-1 for the token that appeared at position i\n",
        "        # We need to handle the case where token_id is out of bounds\n",
        "        if i-1 < token_probs.size(1) and token_id < token_probs.size(2):\n",
        "            token_prob = token_probs[0, i-1, token_id].item()\n",
        "            probabilities.append(token_prob)\n",
        "\n",
        "    # Return average probability across all tokens\n",
        "    return sum(probabilities) / len(probabilities) if probabilities else 0.0\n",
        "\n",
        "def dgrpo_reward_func(prompts, completions, answer, model=None, tokenizer=None, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    DGRPO reward function that promotes diversity in correct solutions.\n",
        "\n",
        "    Args:\n",
        "        prompts: List of input prompts\n",
        "        completions: List of model completions\n",
        "        answer: List of ground truth answers\n",
        "        model: The language model (needed to calculate token probabilities)\n",
        "        tokenizer: The tokenizer (needed to calculate token probabilities)\n",
        "\n",
        "    Returns:\n",
        "        List of rewards with diversity component\n",
        "    \"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    # Calculate correctness (base reward)\n",
        "    correct = [r == a for r, a in zip(extracted_responses, answer)]\n",
        "    base_rewards = [2.0 if c else 0.0 for c in correct]\n",
        "\n",
        "    # If model and tokenizer aren't provided, we can't calculate diversity rewards\n",
        "    if model is None or tokenizer is None:\n",
        "        return base_rewards\n",
        "\n",
        "    # Calculate token probabilities for correct solutions\n",
        "    probabilities = []\n",
        "    for i, response in enumerate(responses):\n",
        "        if correct[i]:\n",
        "            prob = get_token_probabilities(response, model, tokenizer)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    # If no correct solutions, return base rewards\n",
        "    if not probabilities:\n",
        "        return base_rewards\n",
        "\n",
        "    # Calculate average probability across correct solutions\n",
        "    avg_prob = sum(probabilities) / len(probabilities)\n",
        "\n",
        "    # Calculate diversity rewards\n",
        "    final_rewards = []\n",
        "    for i, base_reward in enumerate(base_rewards):\n",
        "        if correct[i]:\n",
        "            # Only calculate diversity bonus for correct solutions\n",
        "            response_prob = get_token_probabilities(responses[i], model, tokenizer)\n",
        "\n",
        "            # Inverse relationship: lower probability = higher diversity bonus\n",
        "            # Scale from 0.0 to 1.0 where 1.0 is max diversity (lowest probability)\n",
        "            if avg_prob > 0:\n",
        "                diversity_bonus = max(0.0, 1.0 - (response_prob / avg_prob))\n",
        "                # Add scaled diversity bonus (0.5 maximum)\n",
        "                final_rewards.append(base_reward + (0.5 * diversity_bonus))\n",
        "            else:\n",
        "                final_rewards.append(base_reward)\n",
        "        else:\n",
        "            final_rewards.append(base_reward)\n",
        "\n",
        "    return final_rewards\n",
        "\n",
        "# Example of using just the reward function with GRPO\n",
        "\n",
        "# Create the standard GRPO Trainer with DGRPO reward function\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        lambda **kwargs: dgrpo_reward_func(model=model, tokenizer=tokenizer, **kwargs),\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "NOcd_veYHaTA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Start the Training Run!\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "JbLM4LuNIRDA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Testing with Small Dataset\n",
        "\n",
        "For faster experimentation and testing of DGRPO, we can use a smaller dataset of just 50 examples.\n",
        "This allows for rapid iterations while testing different reward configurations."
      ],
      "metadata": {
        "id": "r-8X-onjIVrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_small_test_dataset(full_dataset, num_examples=50):\n",
        "    \"\"\"\n",
        "    Create a small test dataset for quick experimentation.\n",
        "\n",
        "    Args:\n",
        "        full_dataset: The complete dataset\n",
        "        num_examples: Number of examples to include in test dataset\n",
        "\n",
        "    Returns:\n",
        "        Small dataset for testing\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    random.seed(42)\n",
        "\n",
        "    # Select random indices for test dataset\n",
        "    if len(full_dataset) <= num_examples:\n",
        "        return full_dataset\n",
        "\n",
        "    indices = random.sample(range(len(full_dataset)), num_examples)\n",
        "    test_dataset = full_dataset.select(indices)\n",
        "\n",
        "    print(f\"Created test dataset with {len(test_dataset)} examples\")\n",
        "    return test_dataset\n",
        "\n",
        "# Create a small test dataset\n",
        "test_dataset = create_small_test_dataset(dataset, num_examples=50)\n",
        "\n",
        "# Configure training arguments for quick testing\n",
        "test_training_args = GRPOConfig(\n",
        "    use_vllm = True,                           # Enable faster inference using vLLM\n",
        "    learning_rate = 5e-6,                      # Small learning rate for stable training\n",
        "    adam_beta1 = 0.9,                          # AdamW optimizer momentum parameter\n",
        "    adam_beta2 = 0.99,                         # AdamW optimizer second moment parameter\n",
        "    weight_decay = 0.1,                        # L2 regularization to prevent overfitting\n",
        "    warmup_ratio = 0.1,                        # Portion of training steps for learning rate warmup\n",
        "    lr_scheduler_type = \"cosine\",              # Learning rate decay schedule type\n",
        "    optim = \"adamw_8bit\",                      # Use 8-bit AdamW optimizer for memory efficiency\n",
        "    logging_steps = 1,                         # Log metrics every step\n",
        "    bf16 = is_bfloat16_supported(),            # Use bfloat16 if hardware supports it\n",
        "    fp16 = not is_bfloat16_supported(),        # Fallback to float16 if bfloat16 not supported\n",
        "    per_device_train_batch_size = 1,           # Number of prompts per GPU\n",
        "    gradient_accumulation_steps = 1,           # Number of steps to accumulate gradients\n",
        "    num_generations = 8,                       # Number of responses to generate per prompt for GRPO\n",
        "    max_prompt_length = 256,                   # Maximum length of input prompts in tokens\n",
        "    max_completion_length = 512,               # Maximum length of model responses in tokens\n",
        "    max_steps = 50,                            # Reduced number of training steps\n",
        "    save_steps = 10,                           # Save checkpoint every 10 steps\n",
        "    max_grad_norm = 0.1,                       # Gradient clipping threshold\n",
        "    report_to = \"wandb\",                       # Log metrics to Weights & Biases\n",
        "    output_dir = \"/content/drive/MyDrive/grpo_model/test_outputs\",  # Different output directory for test runs\n",
        ")\n",
        "\n",
        "# Create test trainer with DGRPO reward function\n",
        "test_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        lambda **kwargs: dgrpo_reward_func(model=model, tokenizer=tokenizer, **kwargs),\n",
        "    ],\n",
        "    args=test_training_args,\n",
        "    train_dataset=test_dataset,\n",
        ")\n"
      ],
      "metadata": {
        "id": "CXqe1C27IOgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad86224-b886-4f33-ccb0-917e0594b526"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created test dataset with 50 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run short test training\n",
        "# # Uncomment to run the test training\n",
        "# print(\"Starting test training run with 50 examples...\")\n",
        "# test_trainer.train()\n",
        "# print(\"Test training completed!\")\n",
        "\n",
        "# # Validate the model after test training\n",
        "# test_prompt = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four eggs. She sells the remainder at the farmers' market daily for $2 per egg. How much in dollars does she make every day at the farmers' market?\"\n",
        "\n",
        "# test_system_prompt = '''\n",
        "# Respond in the following format:\n",
        "# <reasoning>\n",
        "# ...\n",
        "# </reasoning>\n",
        "# <answer>\n",
        "# ...\n",
        "# </answer>\n",
        "# '''\n",
        "\n",
        "# test_text = tokenizer.apply_chat_template([\n",
        "#     {\"role\": \"system\", \"content\": test_system_prompt},\n",
        "#     {\"role\": \"user\", \"content\": test_prompt},\n",
        "# ], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# test_sampling_params = SamplingParams(\n",
        "#     temperature=0.8,\n",
        "#     top_p=0.95,\n",
        "#     max_tokens=1024,\n",
        "# )\n",
        "\n",
        "# test_output = model.fast_generate(\n",
        "#     test_text,\n",
        "#     sampling_params=test_sampling_params,\n",
        "# )[0].outputs[0].text\n",
        "\n",
        "# print(\"\\n\\nTest question:\", test_prompt)\n",
        "# print(\"\\n\\nModel response:\", test_output)\n",
        "\n",
        "# # Save test model (optional)\n",
        "# # Save the LoRA adapter from test run\n",
        "# test_lora_dir = \"/content/drive/MyDrive/grpo_model/test_lora\"\n",
        "# os.makedirs(test_lora_dir, exist_ok=True)\n",
        "# model.save_lora(test_lora_dir)\n",
        "# print(f\"Test LoRA adapter saved to: {test_lora_dir}\")"
      ],
      "metadata": {
        "id": "0ALpKXXuJtJs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Create a copy of the model for standard GRPO training\n",
        "def create_model_copy():\n",
        "    model_copy, tokenizer_copy = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "        fast_inference = True,\n",
        "        max_lora_rank = 64,\n",
        "        gpu_memory_utilization = 0.4,\n",
        "    )\n",
        "\n",
        "    model_copy = FastLanguageModel.get_peft_model(\n",
        "        model_copy,\n",
        "        r = 64,\n",
        "        target_modules = [\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "        lora_alpha = 64,\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 3407,\n",
        "    )\n",
        "\n",
        "    return model_copy, tokenizer_copy\n",
        "\n",
        "# Function to train standard GRPO model\n",
        "def train_grpo():\n",
        "    print(\"Creating model for standard GRPO...\")\n",
        "    standard_model, standard_tokenizer = create_model_copy()\n",
        "\n",
        "    # Configure standard GRPO trainer\n",
        "    comparison_dir = \"/content/drive/MyDrive/grpo_model/comparison_outputs\"\n",
        "    os.makedirs(comparison_dir, exist_ok=True)\n",
        "\n",
        "    standard_args = copy.deepcopy(test_training_args)\n",
        "    standard_args.output_dir = f\"{comparison_dir}/standard_grpo\"\n",
        "    standard_trainer = GRPOTrainer(\n",
        "        model=standard_model,\n",
        "        processing_class=standard_tokenizer,\n",
        "        reward_funcs=[\n",
        "            xmlcount_reward_func,\n",
        "            soft_format_reward_func,\n",
        "            strict_format_reward_func,\n",
        "            int_reward_func,\n",
        "            correctness_reward_func,  # Standard correctness reward\n",
        "        ],\n",
        "        args=standard_args,\n",
        "        train_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    # Run standard GRPO training\n",
        "    print(\"Starting standard GRPO training...\")\n",
        "    standard_results = standard_trainer.train()\n",
        "    standard_logs = copy.deepcopy(standard_trainer.state.log_history)\n",
        "\n",
        "    # Save model to disk before dropping it\n",
        "    standard_dir = \"/content/drive/MyDrive/grpo_model/standard_grpo\"\n",
        "    os.makedirs(standard_dir, exist_ok=True)\n",
        "    standard_model.save_lora(standard_dir)\n",
        "    print(f\"Standard GRPO model saved to: {standard_dir}\")\n",
        "\n",
        "    # Explicitly delete the model to free memory\n",
        "    del standard_model, standard_trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"logs\": standard_logs,\n",
        "        \"model_path\": standard_dir,\n",
        "        \"tokenizer\": standard_tokenizer\n",
        "    }\n",
        "\n",
        "# Function to train DGRPO model\n",
        "def train_dgrpo():\n",
        "    print(\"Creating model for DGRPO...\")\n",
        "    dgrpo_model, dgrpo_tokenizer = create_model_copy()\n",
        "\n",
        "    # Configure DGRPO trainer\n",
        "    comparison_dir = \"/content/drive/MyDrive/grpo_model/comparison_outputs\"\n",
        "    os.makedirs(comparison_dir, exist_ok=True)\n",
        "\n",
        "    dgrpo_args = copy.deepcopy(test_training_args)\n",
        "    dgrpo_args.output_dir = f\"{comparison_dir}/dgrpo\"\n",
        "    dgrpo_trainer = GRPOTrainer(\n",
        "        model=dgrpo_model,\n",
        "        processing_class=dgrpo_tokenizer,\n",
        "        reward_funcs=[\n",
        "            xmlcount_reward_func,\n",
        "            soft_format_reward_func,\n",
        "            strict_format_reward_func,\n",
        "            int_reward_func,\n",
        "            lambda **kwargs: dgrpo_reward_func(model=dgrpo_model, tokenizer=dgrpo_tokenizer, **kwargs),\n",
        "        ],\n",
        "        args=dgrpo_args,\n",
        "        train_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    # Run DGRPO training\n",
        "    print(\"Starting DGRPO training...\")\n",
        "    dgrpo_results = dgrpo_trainer.train()\n",
        "    dgrpo_logs = copy.deepcopy(dgrpo_trainer.state.log_history)\n",
        "\n",
        "    # Save model to disk before dropping it\n",
        "    dgrpo_dir = \"/content/drive/MyDrive/grpo_model/dgrpo\"\n",
        "    os.makedirs(dgrpo_dir, exist_ok=True)\n",
        "    dgrpo_model.save_lora(dgrpo_dir)\n",
        "    print(f\"DGRPO model saved to: {dgrpo_dir}\")\n",
        "\n",
        "    # Explicitly delete the model to free memory\n",
        "    del dgrpo_model, dgrpo_trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"logs\": dgrpo_logs,\n",
        "        \"model_path\": dgrpo_dir,\n",
        "        \"tokenizer\": dgrpo_tokenizer\n",
        "    }\n",
        "\n",
        "# Function to plot the comparison\n",
        "def plot_comparison(comparison_results):\n",
        "    # Extract logs\n",
        "    standard_logs = pd.DataFrame(comparison_results[\"standard\"][\"logs\"])\n",
        "    dgrpo_logs = pd.DataFrame(comparison_results[\"dgrpo\"][\"logs\"])\n",
        "\n",
        "    # Filter only training steps\n",
        "    standard_logs = standard_logs[standard_logs['loss'].notna()]\n",
        "    dgrpo_logs = dgrpo_logs[dgrpo_logs['loss'].notna()]\n",
        "\n",
        "    # Create comparison plots\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Loss comparison\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(standard_logs['step'], standard_logs['loss'], label='Standard GRPO')\n",
        "    plt.plot(dgrpo_logs['step'], dgrpo_logs['loss'], label='DGRPO')\n",
        "    plt.title('Training Loss Comparison')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Reward comparison\n",
        "    if 'reward/mean' in standard_logs.columns and 'reward/mean' in dgrpo_logs.columns:\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(standard_logs['step'], standard_logs['reward/mean'], label='Standard GRPO')\n",
        "        plt.plot(dgrpo_logs['step'], dgrpo_logs['reward/mean'], label='DGRPO')\n",
        "        plt.title('Mean Reward Comparison')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.ylabel('Mean Reward')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    # Plot 3: Learning rate\n",
        "    if 'learning_rate' in standard_logs.columns and 'learning_rate' in dgrpo_logs.columns:\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.plot(standard_logs['step'], standard_logs['learning_rate'], label='Standard GRPO')\n",
        "        plt.plot(dgrpo_logs['step'], dgrpo_logs['learning_rate'], label='DGRPO')\n",
        "        plt.title('Learning Rate')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    # Plot 4: Final performance comparison\n",
        "    plt.subplot(2, 2, 4)\n",
        "    metrics = ['Final Loss', 'Final Reward']\n",
        "    standard_values = [\n",
        "        standard_logs['loss'].iloc[-1],\n",
        "        standard_logs['reward/mean'].iloc[-1] if 'reward/mean' in standard_logs.columns else 0\n",
        "    ]\n",
        "    dgrpo_values = [\n",
        "        dgrpo_logs['loss'].iloc[-1],\n",
        "        dgrpo_logs['reward/mean'].iloc[-1] if 'reward/mean' in dgrpo_logs.columns else 0\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, standard_values, width, label='Standard GRPO')\n",
        "    plt.bar(x + width/2, dgrpo_values, width, label='DGRPO')\n",
        "    plt.title('Final Performance Comparison')\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    comparison_dir = \"/content/drive/MyDrive/grpo_model/comparison_outputs\"\n",
        "    plt.savefig(f\"{comparison_dir}/grpo_vs_dgrpo_comparison.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print out some statistics\n",
        "    print(\"\\nStatistical Comparison:\")\n",
        "    print(\"======================\")\n",
        "    print(f\"Standard GRPO Final Loss: {standard_logs['loss'].iloc[-1]:.4f}\")\n",
        "    print(f\"DGRPO Final Loss: {dgrpo_logs['loss'].iloc[-1]:.4f}\")\n",
        "\n",
        "    if 'reward/mean' in standard_logs.columns and 'reward/mean' in dgrpo_logs.columns:\n",
        "        print(f\"Standard GRPO Final Mean Reward: {standard_logs['reward/mean'].iloc[-1]:.4f}\")\n",
        "        print(f\"DGRPO Final Mean Reward: {dgrpo_logs['reward/mean'].iloc[-1]:.4f}\")\n",
        "\n",
        "        # Calculate relative improvement\n",
        "        reward_improvement = ((dgrpo_logs['reward/mean'].iloc[-1] - standard_logs['reward/mean'].iloc[-1]) /\n",
        "                             standard_logs['reward/mean'].iloc[-1]) * 100\n",
        "        print(f\"DGRPO Reward Improvement: {reward_improvement:.2f}%\")\n",
        "\n",
        "    # Calculate loss improvement\n",
        "    loss_improvement = ((standard_logs['loss'].iloc[-1] - dgrpo_logs['loss'].iloc[-1]) /\n",
        "                        standard_logs['loss'].iloc[-1]) * 100\n",
        "    print(f\"DGRPO Loss Improvement: {loss_improvement:.2f}%\")\n",
        "\n",
        "# Function to test models on a specific problem\n",
        "def test_models(comparison_results):\n",
        "    # Test problem\n",
        "    test_prompt = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four eggs. She sells the remainder at the farmers' market daily for $2 per egg. How much in dollars does she make every day at the farmers' market?\"\n",
        "\n",
        "    correct_answer = \"18\"  # (16 - 3 - 4) * 2 = 18\n",
        "\n",
        "    test_system_prompt = '''\n",
        "    Respond in the following format:\n",
        "    <reasoning>\n",
        "    ...\n",
        "    </reasoning>\n",
        "    <answer>\n",
        "    ...\n",
        "    </answer>\n",
        "    '''\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Test standard GRPO model\n",
        "    print(\"Loading standard GRPO model for testing...\")\n",
        "    standard_model, standard_tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        fast_inference=True,\n",
        "        max_lora_rank=64,\n",
        "        gpu_memory_utilization=0.4,\n",
        "    )\n",
        "\n",
        "    # Load the saved LoRA weights\n",
        "    standard_model = FastLanguageModel.get_peft_model(\n",
        "        standard_model,\n",
        "        r=64,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "        lora_alpha=64,\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "    )\n",
        "\n",
        "    # Load the saved LoRA adapter\n",
        "    standard_model.load_adapter(comparison_results[\"standard\"][\"model_path\"])\n",
        "\n",
        "    tokenizer = comparison_results[\"standard\"][\"tokenizer\"]\n",
        "\n",
        "    text = tokenizer.apply_chat_template([\n",
        "        {\"role\": \"system\", \"content\": test_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": test_prompt},\n",
        "    ], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    output = standard_model.fast_generate(\n",
        "        text,\n",
        "        sampling_params=sampling_params,\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    standard_answer = extract_xml_answer(output)\n",
        "    results[\"standard\"] = {\n",
        "        \"output\": output,\n",
        "        \"extracted_answer\": standard_answer,\n",
        "        \"is_correct\": standard_answer == correct_answer\n",
        "    }\n",
        "\n",
        "    # Delete standard model to free memory before loading DGRPO model\n",
        "    del standard_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Test DGRPO model\n",
        "    print(\"Loading DGRPO model for testing...\")\n",
        "    dgrpo_model, dgrpo_tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        fast_inference=True,\n",
        "        max_lora_rank=64,\n",
        "        gpu_memory_utilization=0.4,\n",
        "    )\n",
        "\n",
        "    # Load the saved LoRA weights\n",
        "    dgrpo_model = FastLanguageModel.get_peft_model(\n",
        "        dgrpo_model,\n",
        "        r=64,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "        lora_alpha=64,\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "    )\n",
        "\n",
        "    # Load the saved LoRA adapter\n",
        "    dgrpo_model.load_adapter(comparison_results[\"dgrpo\"][\"model_path\"])\n",
        "\n",
        "    tokenizer = comparison_results[\"dgrpo\"][\"tokenizer\"]\n",
        "\n",
        "    text = tokenizer.apply_chat_template([\n",
        "        {\"role\": \"system\", \"content\": test_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": test_prompt},\n",
        "    ], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    output = dgrpo_model.fast_generate(\n",
        "        text,\n",
        "        sampling_params=sampling_params,\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    dgrpo_answer = extract_xml_answer(output)\n",
        "    results[\"dgrpo\"] = {\n",
        "        \"output\": output,\n",
        "        \"extracted_answer\": dgrpo_answer,\n",
        "        \"is_correct\": dgrpo_answer == correct_answer\n",
        "    }\n",
        "\n",
        "    # Delete DGRPO model to free memory\n",
        "    del dgrpo_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"=============\")\n",
        "    print(f\"Test Question: {test_prompt}\")\n",
        "    print(f\"Correct Answer: {correct_answer}\")\n",
        "    print(\"\\nStandard GRPO Response:\")\n",
        "    print(results[\"standard\"][\"output\"])\n",
        "    print(f\"Extracted Answer: {results['standard']['extracted_answer']}\")\n",
        "    print(f\"Is Correct: {results['standard']['is_correct']}\")\n",
        "\n",
        "    print(\"\\nDGRPO Response:\")\n",
        "    print(results[\"dgrpo\"][\"output\"])\n",
        "    print(f\"Extracted Answer: {results['dgrpo']['extracted_answer']}\")\n",
        "    print(f\"Is Correct: {results['dgrpo']['is_correct']}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "oi_n4kBYIbiz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison between standard GRPO and DGRPO"
      ],
      "metadata": {
        "id": "8gLqipBAS7oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the comparison (uncomment to execute)\n",
        "print(\"Starting comparison between standard GRPO and DGRPO...\")\n",
        "# Train each model separately\n",
        "print(\"Starting standard GRPO training...\")\n",
        "standard_results = train_grpo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "66bb45c4a6004f538fd7471e598ef5d6",
            "bd1e8ea656ef4fcf803b726475d5fe1d",
            "6bb91d0044db47e095f8706a12b86928",
            "4a4b07aae69142c586655347dae4b843",
            "0c9ea8584cf94ad1a45676bf47e3a158",
            "323a822274e1424984277d8408be6f10",
            "61ae1958579b489d800e8deb70bf9e19",
            "30cb440cea2d4c609bb54b6cad72420a",
            "ccba1f9832b64683b95befe97b88c122",
            "7d20bd32a85e4268a16ca2952dffd067",
            "88bb3d3b38d84bf2a1bdd7699d553f90",
            "0d793345c2f049bb94562e27f2838959",
            "f2ab9d9f5be24b5397eb24e1d7c6dabe",
            "0861df59a45846668df69ad8452de673",
            "ef157a89ec5c4a8e94012519e46544f4",
            "469f4b2420e5445999c81f66ebf0b7f2",
            "5e49f9ee5d7945359e91551d3d3901ed",
            "3b9c5494644c429a8e65ebb076ceae22",
            "066205d757254a3faecde35f99d43da1",
            "0918513db65f4607aa4342e07079228a",
            "75b0fb832232492d886f959d54ed3a46",
            "a404ad22697a487abaa296dcb458c59e"
          ]
        },
        "id": "a2lq2_7MSzcf",
        "outputId": "78f9fceb-1c59-48c3-9c3b-704b8d8a266f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comparison between standard GRPO and DGRPO...\n",
            "Starting standard GRPO training...\n",
            "Creating model for standard GRPO...\n",
            "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.8.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 23.82%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 7.01 GB. Also swap space = 6 GB.\n",
            "INFO 03-23 23:26:46 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 03-23 23:26:46 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
            "INFO 03-23 23:26:47 [model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 03-23 23:26:48 [loader.py:1137] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 03-23 23:26:48 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
            "INFO 03-23 23:26:48 [weight_utils.py:307] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66bb45c4a6004f538fd7471e598ef5d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d793345c2f049bb94562e27f2838959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-23 23:26:51 [model_runner.py:1146] Model loading took 2.4314 GB and 2.719629 seconds\n",
            "INFO 03-23 23:26:52 [worker.py:267] Memory profiling takes 0.95 seconds\n",
            "INFO 03-23 23:26:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.24) = 9.42GiB\n",
            "INFO 03-23 23:26:52 [worker.py:267] model weights take 2.43GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.04GiB; the rest of the memory reserved for KV Cache is 5.95GiB.\n",
            "INFO 03-23 23:26:53 [executor_base.py:111] # cuda blocks: 10829, # CPU blocks: 10922\n",
            "INFO 03-23 23:26:53 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 84.60x\n",
            "INFO 03-23 23:26:58 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 27/27 [00:45<00:00,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-23 23:27:44 [model_runner.py:1570] Graph capturing finished in 46 secs, took 0.56 GiB\n",
            "INFO 03-23 23:27:44 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 52.67 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting standard GRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 50 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
            " \"-____-\"     Trainable parameters = 119,734,272/3,000,000,000 (3.99% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mburian-lib\u001b[0m (\u001b[33mburian-lib-libor-burian\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250323_232806-kz655oce</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/burian-lib-libor-burian/huggingface/runs/kz655oce' target=\"_blank\">/content/drive/MyDrive/grpo_model/test_outputs</a></strong> to <a href='https://wandb.ai/burian-lib-libor-burian/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/burian-lib-libor-burian/huggingface' target=\"_blank\">https://wandb.ai/burian-lib-libor-burian/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/burian-lib-libor-burian/huggingface/runs/kz655oce' target=\"_blank\">https://wandb.ai/burian-lib-libor-burian/huggingface/runs/kz655oce</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "The Rotary Club is holding its annual fundraising Omelet Breakfast, with tickets sold in advance. The tickets come in different price levels, for young children, older children, adults, and seniors. This year they sold 53 small children tickets, 35 older children tickets, 75 adult tickets, and 37 senior tickets. To figure out how many eggs they need to buy, the club estimates that small children can eat a half omelet, older children can eat a whole omelet, adults will eat two omelets, and seniors will eat one and a half omelets. Just to be on the safe side, they get enough eggs to make 25 extra omelets. If they use 2 eggs for each omelet, how many eggs will they need to buy? \n",
            "Answer:\n",
            "584 \n",
            "Response:\n",
            "<reasoning>\n",
            "To determine the total number of eggs the Rotary Club will need to buy, let's break down the problem into smaller steps:\n",
            "- First, we need to calculate the total number of omelets needed based on the number of tickets sold and the given consumption rates.\n",
            "- We include an additional 25 extra omelets just in case.\n",
            "- Then, we'll multiply the total number of omelets by 2 (since each omelet requires 2 eggs).\n",
            "\n",
            "Let's calculate the consumption for each age group:\n",
            "- Small children: 53 tickets at 0.5 omelet per ticket, total omelets = 53 * 0.5\n",
            "- Older children: 35 tickets at 1 omelet per ticket, total omelets = 35 * 1\n",
            "- Adults: 75 tickets at 2 omelets per ticket, total omelets = 75 * 2\n",
            "- Seniors: 37 tickets at 1.5 omelets per ticket, total omelets = 37 * 1.5\n",
            "\n",
            "We add 25 extra omelets, which will also have 2 eggs each, to cover any unexpected increase in demand. Finally, we'll multiply the total number of omelets by 2 to find the required eggs.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "First, calculating the total omelets based on ticket sales and given consumption rates:\n",
            "- Small children: 53 * 0.5 = 26.5 omelets\n",
            "- Older children: 35 * 1 = 35 omelets\n",
            "- Adults: 75 * 2 = 150 omelets\n",
            "- Seniors: 37 * 1.5 = 55.5 omelets\n",
            "\n",
            "Adding these numbers together gives us the omelets from ticket sales:\n",
            "26.5 + 35 + 150 + 55.5 = 267 omelets\n",
            "\n",
            "Adding the extra 25 omelets just in case:\n",
            "267 + 25 = 292 omelets\n",
            "\n",
            "Since 2 eggs are needed for each omelet, the total number of eggs required is:\n",
            "292 * 2 = 584 eggs\n",
            "\n",
            "Therefore, the Rotary Club will need to buy 584 eggs.\n",
            "</answer> \n",
            "Extracted:\n",
            "First, calculating the total omelets based on ticket sales and given consumption rates:\n",
            "- Small children: 53 * 0.5 = 26.5 omelets\n",
            "- Older children: 35 * 1 = 35 omelets\n",
            "- Adults: 75 * 2 = 150 omelets\n",
            "- Seniors: 37 * 1.5 = 55.5 omelets\n",
            "\n",
            "Adding these numbers together gives us the omelets from ticket sales:\n",
            "26.5 + 35 + 150 + 55.5 = 267 omelets\n",
            "\n",
            "Adding the extra 25 omelets just in case:\n",
            "267 + 25 = 292 omelets\n",
            "\n",
            "Since 2 eggs are needed for each omelet, the total number of eggs required is:\n",
            "292 * 2 = 584 eggs\n",
            "\n",
            "Therefore, the Rotary Club will need to buy 584 eggs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 08:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / xmlcount_reward_func</th>\n",
              "      <th>rewards / soft_format_reward_func</th>\n",
              "      <th>rewards / strict_format_reward_func</th>\n",
              "      <th>rewards / int_reward_func</th>\n",
              "      <th>rewards / correctness_reward_func</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.779000</td>\n",
              "      <td>0.177374</td>\n",
              "      <td>437.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.779000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.313250</td>\n",
              "      <td>0.082907</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.313250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>1.048524</td>\n",
              "      <td>238.375000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>-0.314500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.055375</td>\n",
              "      <td>0.141669</td>\n",
              "      <td>168.250000</td>\n",
              "      <td>0.024520</td>\n",
              "      <td>-0.055375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.443250</td>\n",
              "      <td>0.268932</td>\n",
              "      <td>234.750000</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>-0.443250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.248375</td>\n",
              "      <td>0.215769</td>\n",
              "      <td>219.875000</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>-0.248375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.502125</td>\n",
              "      <td>0.221437</td>\n",
              "      <td>308.750000</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>-0.502125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.087875</td>\n",
              "      <td>1.260800</td>\n",
              "      <td>359.250000</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>-0.162125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.063250</td>\n",
              "      <td>0.867261</td>\n",
              "      <td>252.000000</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>-0.375750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.054625</td>\n",
              "      <td>0.180573</td>\n",
              "      <td>178.125000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>-0.054625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.445125</td>\n",
              "      <td>0.329700</td>\n",
              "      <td>256.250000</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>-0.445125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036375</td>\n",
              "      <td>0.142473</td>\n",
              "      <td>135.250000</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.036375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.261500</td>\n",
              "      <td>0.894288</td>\n",
              "      <td>343.875000</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>-0.574000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>0.854798</td>\n",
              "      <td>183.875000</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>-0.225000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.240500</td>\n",
              "      <td>0.912274</td>\n",
              "      <td>305.250000</td>\n",
              "      <td>0.000487</td>\n",
              "      <td>-0.553000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.364125</td>\n",
              "      <td>0.078815</td>\n",
              "      <td>212.375000</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>-0.364125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.012125</td>\n",
              "      <td>0.870517</td>\n",
              "      <td>209.625000</td>\n",
              "      <td>0.001252</td>\n",
              "      <td>-0.300375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.451625</td>\n",
              "      <td>0.325280</td>\n",
              "      <td>309.875000</td>\n",
              "      <td>0.000970</td>\n",
              "      <td>-0.451625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023625</td>\n",
              "      <td>0.938064</td>\n",
              "      <td>237.500000</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>-0.288875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.371000</td>\n",
              "      <td>0.149889</td>\n",
              "      <td>262.625000</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>-0.371000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.331250</td>\n",
              "      <td>1.260833</td>\n",
              "      <td>263.375000</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>-0.293750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.027500</td>\n",
              "      <td>0.660205</td>\n",
              "      <td>275.250000</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>-0.340000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.657500</td>\n",
              "      <td>0.391948</td>\n",
              "      <td>381.000000</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>-0.657500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.033000</td>\n",
              "      <td>0.168721</td>\n",
              "      <td>180.250000</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>-0.033000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.128000</td>\n",
              "      <td>0.107839</td>\n",
              "      <td>215.625000</td>\n",
              "      <td>0.002011</td>\n",
              "      <td>-0.128000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.470500</td>\n",
              "      <td>1.143423</td>\n",
              "      <td>228.625000</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>-0.154500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.692250</td>\n",
              "      <td>0.669489</td>\n",
              "      <td>488.375000</td>\n",
              "      <td>0.025046</td>\n",
              "      <td>-0.692250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.159250</td>\n",
              "      <td>0.249967</td>\n",
              "      <td>210.125000</td>\n",
              "      <td>0.003454</td>\n",
              "      <td>-0.221750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.245250</td>\n",
              "      <td>1.017695</td>\n",
              "      <td>169.500000</td>\n",
              "      <td>0.002804</td>\n",
              "      <td>-0.067250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.908375</td>\n",
              "      <td>1.095891</td>\n",
              "      <td>137.625000</td>\n",
              "      <td>0.003230</td>\n",
              "      <td>0.095875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.178125</td>\n",
              "      <td>0.192171</td>\n",
              "      <td>193.625000</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>-0.178125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.222500</td>\n",
              "      <td>0.110678</td>\n",
              "      <td>218.250000</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>-0.222500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.319875</td>\n",
              "      <td>1.093684</td>\n",
              "      <td>298.750000</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>-0.367625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.048625</td>\n",
              "      <td>0.130440</td>\n",
              "      <td>149.375000</td>\n",
              "      <td>0.002154</td>\n",
              "      <td>0.048625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.059152</td>\n",
              "      <td>157.125000</td>\n",
              "      <td>0.002038</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.240500</td>\n",
              "      <td>1.061599</td>\n",
              "      <td>204.500000</td>\n",
              "      <td>0.004049</td>\n",
              "      <td>-0.259500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.510625</td>\n",
              "      <td>0.352030</td>\n",
              "      <td>344.625000</td>\n",
              "      <td>0.002288</td>\n",
              "      <td>-0.510625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.077875</td>\n",
              "      <td>0.112939</td>\n",
              "      <td>173.250000</td>\n",
              "      <td>0.003290</td>\n",
              "      <td>-0.077875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.368375</td>\n",
              "      <td>0.349515</td>\n",
              "      <td>256.750000</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>-0.368375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.570375</td>\n",
              "      <td>1.154851</td>\n",
              "      <td>148.375000</td>\n",
              "      <td>0.006002</td>\n",
              "      <td>-0.054625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.111750</td>\n",
              "      <td>0.913710</td>\n",
              "      <td>293.875000</td>\n",
              "      <td>0.002862</td>\n",
              "      <td>-0.424250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.272750</td>\n",
              "      <td>0.227223</td>\n",
              "      <td>197.875000</td>\n",
              "      <td>0.004542</td>\n",
              "      <td>-0.272750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>2.065500</td>\n",
              "      <td>0.744778</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>0.003107</td>\n",
              "      <td>-0.184500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.286625</td>\n",
              "      <td>0.510809</td>\n",
              "      <td>329.375000</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>-0.286625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.482500</td>\n",
              "      <td>0.217369</td>\n",
              "      <td>325.250000</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>-0.482500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.143250</td>\n",
              "      <td>0.966112</td>\n",
              "      <td>226.250000</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.169250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>0.172080</td>\n",
              "      <td>223.500000</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.467500</td>\n",
              "      <td>1.281396</td>\n",
              "      <td>334.250000</td>\n",
              "      <td>0.001771</td>\n",
              "      <td>-0.595000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>0.174124</td>\n",
              "      <td>146.375000</td>\n",
              "      <td>0.003378</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.509625</td>\n",
              "      <td>0.103587</td>\n",
              "      <td>319.125000</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>-0.509625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "Every hour past noon shadows from a building stretch an extra 5 feet, starting at zero at noon.  How long are the shadows from the building 6 hours past noon in inches? \n",
            "Answer:\n",
            "360 \n",
            "Response:\n",
            "<reasoning>\n",
            "At noon, the shadows from the building start stretching, gaining 5 feet of length every hour past noon. By 6 hours past noon, shadows would have stretched for 6 hours. Since each hour they stretch an extra 5 feet, for 6 hours, they would have stretched 6 * 5 feet = 30 feet. Since the question asks for the length of the shadows in inches, we convert the 30 feet to inches. Knowing that one foot equals 12 inches, 30 feet * 12 inches/foot = 360 inches.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "The shadows from the building 6 hours past noon are 360 inches long. </answer> \n",
            "Extracted:\n",
            "The shadows from the building 6 hours past noon are 360 inches long.\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "-------------------- Question:\n",
            "Edward the plumber is replacing a pipe in a bathroom wall.  He needs to use 40 feet of copper pipe to complete the job.  For every 5 feet of pipe, Edward must use one tightening bolt, and for every bolt, he uses two washers.  He buys a bag of 20 washers for the job.  After he completes the bathroom job, how many washers will be remaining in the bag? \n",
            "Answer:\n",
            "4 \n",
            "Response:\n",
            "<reasoning>\n",
            "Edward needs to determine the number of washers he will use based on the 40 feet of copper pipe required. We start by figuring out the number of bolts needed since one bolt is used for every 5 feet of pipe. With 40 feet of pipe, he'll need 40 / 5 = 8 bolts. Next, he uses 2 washers for each bolt. Thus, he will require 8 bolts * 2 washers per bolt = 16 washers. Since he starts with a bag of 20 washers, he will have 20 - 16 washers after using the necessary amount.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "The calculation is 20 - 16 = 4 washers. Therefore, there will be 4 washers remaining in Edward's bag after completing the pipe job.\n",
            "</answer> \n",
            "Extracted:\n",
            "The calculation is 20 - 16 = 4 washers. Therefore, there will be 4 washers remaining in Edward's bag after completing the pipe job.\n",
            "-------------------- Question:\n",
            "The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received. \n",
            "Answer:\n",
            "30 \n",
            "Response:\n",
            "<reasoning>\n",
            "Kendra bought two wooden toys and three hats. The cost of one wooden toy is $20, and the cost of one hat is $啪$10$. So, the total cost for two wooden toys is 2 * $20 = $40. The total cost for three hats is 3 * $10 = $30. Therefore, the total cost for the items Kendra bought is $40 + $30 = $70. Given that Kendra had a $100 bill, the change she would receive after the purchase can be calculated by subtracting the total cost from $100, which is $100 - $70 = $30.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Kendra received $30$ in change after her purchases.\n",
            "</answer> \n",
            "Extracted:\n",
            "Kendra received $30$ in change after her purchases.\n",
            "-------------------- Question:\n",
            "Barry stands on his head for 10 minutes at a time, but then he must sit for 5 minutes before he can take another turn standing on his head.  How many turns can Barry take standing on his head during a single 2-hour period? \n",
            "Answer:\n",
            "8 \n",
            "Response:\n",
            "<reasoning>\n",
            "Barry's cycle consists of 10 minutes standing on his head followed by 5 minutes resting. This forms a 15-minute cycle (10 + 5). We need to determine how many complete 15-minute cycles can fit into a 2-hour (120-minute) period. We can find how many of these cycles can occur during the 2-hour session by dividing the total time by the length of one cycle. \n",
            "\n",
            "\\[\n",
            "\\text{Number of cycles} = \\frac{\\text{Total time available (in minutes)}}{\\text{Duration of one cycle (in minutes)}}\n",
            "\\]\n",
            "\\[\n",
            "\\text{Number of cycles} = \\frac{120 \\text{ minutes}}{15 \\text{ minutes/cycle}}\n",
            "\\]\n",
            "\\[\n",
            "\\text{Number of cycles} = 8\n",
            "\\]\n",
            "Since a complete cycle of 10 minutes standing and 5 minutes resting indicates one turn standing on his head, Barry gets 8 turns standing on his head during a 2-hour period.\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "Barry can take 8 turns standing on his head during a single 2-hour period. </answer> \n",
            "Extracted:\n",
            "Barry can take 8 turns standing on his head during a single 2-hour period.\n",
            "-------------------- Question:\n",
            "Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year? \n",
            "Answer:\n",
            "130 \n",
            "Response:\n",
            "<reasoning>\n",
            "In the first year, Aaron pays $80 for his membership. Each subsequent year, he pays $10 more than he paid the previous year. To find out how much his membership costs in the sixth year, we can calculate the increase of fees over the five years from the first year to the sixth year, and add that to the initial fee.\n",
            "\n",
            "The increase in fee per year is $10. Over 5 years, the total increase would be \\(5 \\times 10 = 50\\) dollars. Adding this to the initial fee of $80, the total fee Aaron pays in the sixth year would be \\(80 + 50 = 130\\) dollars.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "$130\n",
            "</answer> \n",
            "Extracted:\n",
            "$130\n",
            "-------------------- Question:\n",
            "James is trying to create a new breed of kittens with extra-long tails. Each generation of kittens he breeds has a tail 25% longer than the last generation. If the first generation has tails 16 cm long, how long are the third generation's tails? \n",
            "Answer:\n",
            "25 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find the length of the tails in the third generation, we need to calculate how much the tail length increases from the first generation to the third generation, given that it increases by 25% each generation. \n",
            "\n",
            "The length of the tails in the first generation is given as 撅16 cm. The tail length increases by 25% from one generation to the next, which means we need to apply this increase twice (from the first to the second generation and then from the second to the third generation).\n",
            "\n",
            "First, the length of the tails in the second generation will be the length of the first generation's tails plus a 25% increase:\n",
            "\\[ \\text{Second generation tail length} = \\text{First generation tail length} + 0.25 \\times \\text{First generation tail length} \\]\n",
            "\\[ \\text{Second generation tail length} = 16 + 16 \\times 0.25 = 16 + 4 = 20 \\text{ cm} \\]\n",
            "\n",
            "Next, we find the length of the tails in the third generation by increasing the length of the second generation's tails by 25%:\n",
            "\\[ \\text{Third generation tail length} = \\text{Second generation tail length} + 0.25 \\times \\text{Second generation tail length} \\]\n",
            "\\[ \\text{Third generation tail length} = 20 + 20 \\times 0.25 = 20 + 5 = 25 \\text{ cm} \\]\n",
            "\n",
            "So, the length of the tails in the third generation is 25 cm.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "The length of the tails in the third generation is 25 cm.\n",
            "</answer> \n",
            "Extracted:\n",
            "The length of the tails in the third generation is 25 cm.\n",
            "-------------------- Question:\n",
            "By the time Anne is two times as old as Emile, Emile will be six times as old as Maude. If Maude will be 8 years old, how old will Anne be? \n",
            "Answer:\n",
            "96 \n",
            "Response:\n",
            "<reasoning>\n",
            "Let's break down the problem step by step. We know that Maude will be 8 years old. According to the problem, by the time Anne is two times as old as Emile, Emile will be six times as old as Maude. This gives us the following information, which we can express as equations:\n",
            "\n",
            "1. Let \\( A \\) be Anne's age, \\( E \\) be Emile's age, and \\( Ma \\) be Maude's age (which is 8 years).\n",
            "2. We know that \\( Ma = 8 \\).\n",
            "3. The statement \"By the time Anne is two times as old as Emile, Emile will be six times as old as Maude\" translates to:\n",
            "   - When Anne's future age is \\( A_f \\), Emile's future age is \\( E_f \\). At that time, \\( E_f = 6 \\times Ma \\).\n",
            "   - And \\( A_f = 2 \\times E_f \\).\n",
            "\n",
            "From \\( Ma = 8 \\):\n",
            "\\[ E_f = 6 \\times 8 = 48. \\]\n",
            "\n",
            "Therefore, Emile's future age will be 48 years.\n",
            "\n",
            "Since \\( A_f = 2 \\times E_f \\):\n",
            "\\[ A_f = 2 \\times 48 = 96. \\]\n",
            "\n",
            "Now Anne's future age in the described scenario is 96 years. But we need to find Anne's age when this scenario occurs. Let's denote the number of years from now until the described scenario happens as \\( x \\). This makes the following equations hold:\n",
            "\n",
            "\\[ A + x = 2(E + x) \\]\n",
            "\\[ 48 + x = 2E \\]\n",
            "\n",
            "Since \\( E_f = 48 \\), \\( E \\) must be 48 years before \\( E_f \\) in the future occurs. So we solve for \\( x \\):\n",
            "\\[ E = 48 - x. \\]\n",
            "\n",
            "Substitute \\( E \\) into the equation \\( 48 + x = 2(48 - x) \\):\n",
            "\\[ 48 + x = 96 - 2x \\]\n",
            "\\[ 3x = 48 \\]\n",
            "\\[ x = 16. \\]\n",
            "\n",
            "Therefore, Anne's current age is \\( 96 - 16 = 80 \\) years. \n",
            "\n",
            "So, Anne will be 96 years old by the time the given scenario happens.\n",
            "</reason \n",
            "Extracted:\n",
            "<reasoning>\n",
            "Let's break down the problem step by step. We know that Maude will be 8 years old. According to the problem, by the time Anne is two times as old as Emile, Emile will be six times as old as Maude. This gives us the following information, which we can express as equations:\n",
            "\n",
            "1. Let \\( A \\) be Anne's age, \\( E \\) be Emile's age, and \\( Ma \\) be Maude's age (which is 8 years).\n",
            "2. We know that \\( Ma = 8 \\).\n",
            "3. The statement \"By the time Anne is two times as old as Emile, Emile will be six times as old as Maude\" translates to:\n",
            "   - When Anne's future age is \\( A_f \\), Emile's future age is \\( E_f \\). At that time, \\( E_f = 6 \\times Ma \\).\n",
            "   - And \\( A_f = 2 \\times E_f \\).\n",
            "\n",
            "From \\( Ma = 8 \\):\n",
            "\\[ E_f = 6 \\times 8 = 48. \\]\n",
            "\n",
            "Therefore, Emile's future age will be 48 years.\n",
            "\n",
            "Since \\( A_f = 2 \\times E_f \\):\n",
            "\\[ A_f = 2 \\times 48 = 96. \\]\n",
            "\n",
            "Now Anne's future age in the described scenario is 96 years. But we need to find Anne's age when this scenario occurs. Let's denote the number of years from now until the described scenario happens as \\( x \\). This makes the following equations hold:\n",
            "\n",
            "\\[ A + x = 2(E + x) \\]\n",
            "\\[ 48 + x = 2E \\]\n",
            "\n",
            "Since \\( E_f = 48 \\), \\( E \\) must be 48 years before \\( E_f \\) in the future occurs. So we solve for \\( x \\):\n",
            "\\[ E = 48 - x. \\]\n",
            "\n",
            "Substitute \\( E \\) into the equation \\( 48 + x = 2(48 - x) \\):\n",
            "\\[ 48 + x = 96 - 2x \\]\n",
            "\\[ 3x = 48 \\]\n",
            "\\[ x = 16. \\]\n",
            "\n",
            "Therefore, Anne's current age is \\( 96 - 16 = 80 \\) years. \n",
            "\n",
            "So, Anne will be 96 years old by the time the given scenario happens.\n",
            "</reason\n",
            "-------------------- Question:\n",
            "Purple Valley has one-quarter as many skirts as Seafoam Valley, and Seafoam Valley has two-thirds as many skirts as Azure Valley. If Azure Valley has 60 skirts, how many skirts does Purple Valley have? \n",
            "Answer:\n",
            "10 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find out how many skirts Purple Valley has, we can follow these steps:\n",
            "\n",
            "1. Start with the given information. Azure Valley has 野60 skirts.\n",
            "2. From the information, Seafoam Valley has two-thirds as many skirts as Azure Valley. Therefore, to find the number of skirts in Seafoam Valley, we can perform the calculation:\n",
            "\\[ \\text{Skirts in Seafoam Valley} = \\frac{2}{3} \\times \\text{Skirts in Azure Valley} = \\frac{2}{3} \\times 60 = 40 \\]\n",
            "3. Now, we know Seafoam Valley has 40 skirts.\n",
            "4. Purple Valley has one-quarter as many skirts as Seafoam Valley. To find the number of skirts in Purple Valley, we perform:\n",
            "\\[ \\text{Skirts in Purple Valley} = \\frac{1}{4} \\times \\text{Skirts in Seafoam Valley} = \\frac{1}{4} \\times 40 = 10 \\]\n",
            "\n",
            "Therefore, Purple Valley has 10 skirts.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "Purple Valley has 10 skirts.\n",
            "</answer> \n",
            "Extracted:\n",
            "Purple Valley has 10 skirts.\n",
            "-------------------- Question:\n",
            "Jimmy bought 3 pens for school for $1 each, 4 notebooks for $3 each and 2 folders for $5 each. If he paid with a $50 bill, how much change will he get back? \n",
            "Answer:\n",
            "25 \n",
            "Response:\n",
            "<reasoning>\n",
            "Jimmy's expenses include:\n",
            "- Pens: 3 pens * $1 = $3\n",
            "- Notebooks: 4 notebooks * $3 = $12\n",
            "- Folders: 2 folders * $5 = $10\n",
            "\n",
            "His total expenses are $3 + $12 + $10 = $25.\n",
            "He paid with a $50 bill, so to find the change, we subtract the total expense from the bill: $50 - $25 = $25.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "Jimmy will get $25 in change.\n",
            "</answer> \n",
            "Extracted:\n",
            "Jimmy will get $25 in change.\n",
            "-------------------- Question:\n",
            "After complaints from the residents of Tatoosh about the number of cats on the island, the wildlife service carried out a relocation mission that saw the number of cats on the island drastically reduced. On the first relocation mission, 600 cats were relocated from the island to a neighboring island. On the second mission, half of the remaining cats were relocated to a rescue center inland. If the number of cats originally on the island was 1800, how many cats remained on the island after the rescue mission? \n",
            "Answer:\n",
            "600 \n",
            "Response:\n",
            "<reasoning>\n",
            "Initially, there were 180 wild cats on Tatoosh Island. \n",
            "\n",
            "On the first relocation mission, 600 cats were relocated. \n",
            "This left 1800 (original number) - 600 (removed) = 1200 cats.\n",
            "\n",
            "On the second relocation mission, half of the remaining cats were relocated. \n",
            "This means 1200 (remaining after first mission) / 2 = 600 cats were relocated.\n",
            "\n",
            "After the second mission, the number of cats remaining on the island would be 1200 (after first mission) - 600 (after second mission) = 600 cats.\n",
            "\n",
            "So, the number of cats that remained on the island after the rescue mission is 600.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "600 cats remained on the island after the rescue mission. </answer> \n",
            "Extracted:\n",
            "600 cats remained on the island after the rescue mission.\n",
            "-------------------- Question:\n",
            "Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left? \n",
            "Answer:\n",
            "14 \n",
            "Response:\n",
            "<reasoning>\n",
            "Ines spent $2 on each pound of peaches and bought 3 pounds. To find out how much she spent on peaches, we multiply the price per pound by the number of pounds: $2 x 3 = $6. \n",
            "To find out how much money she has left, we subtract the amount she spent on peaches from the amount of money she originally had: $20 - $6 = $14.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Ines had $14 left after purchasing the peaches.\n",
            "</answer> \n",
            "Extracted:\n",
            "Ines had $14 left after purchasing the peaches.\n",
            "-------------------- Question:\n",
            "If Lyn donates $240 to a private organization each year where 1/3 of it goes to the community pantry project, 1/2 goes to the local crisis fund, 1/4 of the remaining goes to livelihood project funds, and the rest is for contingency funds. How much goes to contingency fund? \n",
            "Answer:\n",
            "30 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find out how much of the $240 donation goes to the contingency fund, we need to follow these steps:\n",
            "\n",
            "1. Determine how much of the donation goes to the community pantry project.\n",
            "2. Determine how much goes to the local crisis fund.\n",
            "3. Calculate the remaining amount after the first two funds are subtracted.\n",
            "4. Determine how much goes to the livelihood project funds from the remaining amount.\n",
            "5. Calculate the amount left for the contingency fund.\n",
            "\n",
            "Let's start with the donation of $240.\n",
            "\n",
            "1. The community pantry project receives 1/3 of the donation.\n",
            "   \\[ \\frac{1}{3} \\times 240 = \\frac{240}{3} = 80 \\]\n",
            "   So, the community pantry project gets $80.\n",
            "\n",
            "2. The local crisis fund receives 1/2 of the donation.\n",
            "   \\[ \\frac{1}{2} \\times 240 = \\frac{240}{2} = 120 \\]\n",
            "   Thus, the local crisis fund gets $120.\n",
            "\n",
            "3. The remaining amount is:\n",
            "   \\[ 240 - 80 - 120 = 40 \\]\n",
            "   So, $40 remains after the first two funds are subtracted.\n",
            "\n",
            "4. The livelihood project funds receive 1/4 of the remaining amount.\n",
            "   \\[ \\frac{1}{4} \\times 40 = 10 \\]\n",
            "   Therefore, the livelihood project gets $10.\n",
            "\n",
            "5. Subtract the amount allocated to the livelihood project from the remaining amount to find the contingency fund:\n",
            "   \\[ 40 - 10 = 30 \\]\n",
            "   Hence, the contingency fund gets $30.\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "The amount that goes to the contingency fund is $30. </answer> \n",
            "Extracted:\n",
            "The amount that goes to the contingency fund is $30.\n",
            "-------------------- Question:\n",
            "Grant has four times as many vacations as Kelvin has classes. If Kelvin has 90 classes, how many vacations and classes do Grant and Kelvin have altogether? \n",
            "Answer:\n",
            "450 \n",
            "Response:\n",
            "<reasoning>\n",
            "Let's break down the problem:\n",
            "1. Kelvin has 90 classes.\n",
            "2. Grant has four times as many vacations as Kelvin has classes.\n",
            "Thus, Grant has \\(4 \\times 90 = 360\\) vacations.\n",
            "\n",
            "To find out how many vacations and classes Grant and Kelvin have altogether, we need to add Kelvin's classes to Grant's vacations.\n",
            "\n",
            "Let's calculate the total.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Overall, Grant and Kelvin have 450 vacations and classes together:\n",
            "\\[\n",
            "90 \\text{ (Kelvin's classes)} + 360 \\text{ (Grant's vacations)} = 450\n",
            "\\]\n",
            "\n",
            "Therefore, the total number of vacations and classes they have is 450.\n",
            "</answer> \n",
            "Extracted:\n",
            "Overall, Grant and Kelvin have 450 vacations and classes together:\n",
            "\\[\n",
            "90 \\text{ (Kelvin's classes)} + 360 \\text{ (Grant's vacations)} = 450\n",
            "\\]\n",
            "\n",
            "Therefore, the total number of vacations and classes they have is 450.\n",
            "-------------------- Question:\n",
            "There are 360 balls in the ball pit. If a quarter of the balls in the ball pit are red and a fifth of the remaining balls are blue. How many of the balls are neither red nor blue? \n",
            "Answer:\n",
            "216 \n",
            "Response:\n",
            "<reasoning>\n",
            "To determine the number of balls that are neither red nor blue, we first need to find the number of red and blue balls, and then subtract the sum of these from the total number of balls.\n",
            "\n",
            "1. First, we calculate the number of red balls. Since a quarter of the balls are red, the number of red balls is \\( \\frac{1}{4} \\times 360 \\).\n",
            "2. After removing the red balls, we calculate the remaining number of balls, which is \\( 360 - \\text{number of red balls} \\).\n",
            "3. Then, we determine the number of blue balls, which is \\( \\frac{1}{5} \\) of the remaining balls.\n",
            "4. The number of balls that are neither red nor blue is the total number of balls minus the number of red balls minus the number of blue balls.\n",
            "</reasoning>\n",
            "<answer>\n",
            "First, calculate the number of red balls:\n",
            "\\[\n",
            "\\frac{1}{4} \\times 360 = 90\n",
            "\\]\n",
            "This means 90 red balls out of 360 total balls.\n",
            "\n",
            "Next, calculate the number of balls that are neither red nor blue after the red balls are removed:\n",
            "\\[\n",
            "360 - 90 = 270\n",
            "\\]\n",
            "This leaves 270 balls. Then, calculate the number of blue balls:\n",
            "\\[\n",
            "\\frac{1}{5} \\times 270 = 54\n",
            "\\]\n",
            "So, 54 blue balls are removed.\n",
            "\n",
            "Finally, the total number of balls that are neither red nor blue is:\n",
            "\\[\n",
            "270 - 54 = 216\n",
            "\\]\n",
            "\n",
            "Thus, there are 216 balls that are neither red nor blue in the ball pit.\n",
            "</answer> \n",
            "Extracted:\n",
            "First, calculate the number of red balls:\n",
            "\\[\n",
            "\\frac{1}{4} \\times 360 = 90\n",
            "\\]\n",
            "This means 90 red balls out of 360 total balls.\n",
            "\n",
            "Next, calculate the number of balls that are neither red nor blue after the red balls are removed:\n",
            "\\[\n",
            "360 - 90 = 270\n",
            "\\]\n",
            "This leaves 270 balls. Then, calculate the number of blue balls:\n",
            "\\[\n",
            "\\frac{1}{5} \\times 270 = 54\n",
            "\\]\n",
            "So, 54 blue balls are removed.\n",
            "\n",
            "Finally, the total number of balls that are neither red nor blue is:\n",
            "\\[\n",
            "270 - 54 = 216\n",
            "\\]\n",
            "\n",
            "Thus, there are 216 balls that are neither red nor blue in the ball pit.\n",
            "-------------------- Question:\n",
            "Julie is making caesar salad for a family picnic. At the market, she spends $8 on green lettuce and $6 on red lettuce. If each type of lettuce costs $2 per pound, how many total pounds of lettuce did she buy? \n",
            "Answer:\n",
            "7 \n",
            "Response:\n",
            "<reasoning>\n",
            "Julie spent $8 on green lettuce at a cost of $2 per pound. To find out how many pounds of green lettuce she bought, we divide the total amount spent on green lettuce by the cost per pound:\n",
            "\\[ \\text{Pounds of green lettuce} = \\frac{\\$8}{\\$2/\\text{pound}} = 4 \\text{ pounds} \\]\n",
            "\n",
            "Similarly, Julie spent $6 on red lettuce at a cost of $2 per pound. To find out how many pounds of red lettuce she bought, we divide the total amount spent on red lettuce by the cost per pound:\n",
            "\\[ \\text{Pounds of red lettuce} = \\frac{\\$6}{\\$2/\\text{pound}} = 3 \\text{ pounds} \\]\n",
            "\n",
            "To find the total pounds of lettuce she bought, we add the pounds of green and red lettuce together:\n",
            "\\[ \\text{Total pounds of lettuce} = 4 \\text{ pounds} + 3 \\text{ pounds} = 7 \\text{ pounds} \\]\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "Julie bought a total of 7 pounds of lettuce. </answer> \n",
            "Extracted:\n",
            "Julie bought a total of 7 pounds of lettuce.\n",
            "-------------------- Question:\n",
            "Caleb bought 10 cartons of ice cream and 4 cartons of frozen yoghurt. Each carton of ice cream cost $4 and each carton of frozen yoghurt cost $1. How much more did Caleb spend on ice cream than on frozen yoghurt? \n",
            "Answer:\n",
            "36 \n",
            "Response:\n",
            "<reasoning>\n",
            "To determine how much more Caleb spent on ice cream than on frozen yogurt, we need to follow these steps:\n",
            "\n",
            "1. Calculate the total cost of the ice cream.\n",
            "2. Calculate the total cost of the frozen yogurt.\n",
            "3. Subtract the total cost of the frozen yogurt from the total cost of the ice cream to find the difference.\n",
            "\n",
            "Since each carton of ice cream costs $4 and Caleb bought 10 cartons, the total cost of the ice cream can be calculated as:\n",
            "\\[ \\text{Total cost of ice cream} = 10 \\times \\$4 \\]\n",
            "\n",
            "Similarly, since each carton of frozen yogurt costs $1 and Caleb bought 4 cartons, the total cost of the frozen yogurt can be calculated as:\n",
            "\\[ \\text{Total cost of frozen yogurt} = 4 \\times \\$1 \\]\n",
            "\n",
            "Finally, the difference in the total cost between ice cream and frozen yogurt can be found by subtracting:\n",
            "\\[ \\text{Difference} = \\text{Total cost of ice cream} - \\text{Total cost of frozen yogurt} \\]\n",
            "\n",
            "Now we can substitute and perform the calculations.\n",
            "</reasoning>\n",
            "<answer>\n",
            "10 × $4 for the ice cream = $40\n",
            "4 × $1 for the frozen yogurt = $4\n",
            "\n",
            "Difference = $40 - $4 = $36.\n",
            "\n",
            "Caleb spent $36 more on ice cream than on frozen yogurt.\n",
            "</answer> \n",
            "Extracted:\n",
            "10 × $4 for the ice cream = $40\n",
            "4 × $1 for the frozen yogurt = $4\n",
            "\n",
            "Difference = $40 - $4 = $36.\n",
            "\n",
            "Caleb spent $36 more on ice cream than on frozen yogurt.\n",
            "-------------------- Question:\n",
            "Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked? \n",
            "Answer:\n",
            "40 \n",
            "Response:\n",
            "<reasoning>\n",
            "To solve this problem, we need to determine the total number of strawberries picked by Betty, Matthew, and Natalie, and then calculate the total amount of jam jars they can make. We will use the following steps:\n",
            "\n",
            "1. Calculate the number of strawberries Matthew picked.\n",
            "2. Calculate the number of strawberries Natalie picked.\n",
            "棒\n",
            "2. Sum up the total number of strawberries.\n",
            "3. Calculate the number of jam jars they can make from these strawberries.\n",
            "4. Calculate the total money they made from selling the jars of jam.\n",
            "\n",
            "Let's start with step 1:\n",
            "\n",
            "Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty, so:\n",
            "\\[ 16 + 20 = 36 \\]\n",
            "\n",
            "Matthew picked 36 strawberries. We don't know the exact number of strawberries Natalie picked initially, but we can calculate that. Since Matthew picked twice as many strawberries as Natalie, we can assume Natalie's strawberries as \\( x \\) and Matthew's strawberries as \\( 2x \\). Since Matthew picked 36 strawberries, we have:\n",
            "\\[ 2x = 36 \\]\n",
            "\\[ x = 18 \\]\n",
            "\n",
            "So, Natalie picked 18 strawberries.\n",
            "\n",
            "Now, we move to step 2: sum up the total number of strawberries:\n",
            "\\[ 16 + 36 + 18 = 70 \\]\n",
            "\n",
            "Next, for step 3: Calculate the number of jam jars they can make. One jar of jam uses 7 strawberries, so:\n",
            "\\[ \\frac{70}{7} = 10 \\text{ jars} \\]\n",
            "\n",
            "Finally, for step 4: Calculate the total money they made. Each jar is sold for $4:\n",
            "\\[ 10 \\times 4 = $40 \\]\n",
            "\n",
            "So, the total money they were able to make from the strawberries is $40.\n",
            "</reasoning>\n",
            "<answer>\n",
            "$40\n",
            "</answer> \n",
            "Extracted:\n",
            "$40\n",
            "-------------------- Question:\n",
            "Fern buys one pair of high heels for $60 and five pairs of ballet slippers for 2/3rds of the price of the high heels. How much does she pay total? \n",
            "Answer:\n",
            "260 \n",
            "Response:\n",
            "<reasoning>\n",
            "First, let's calculate the cost of one pair of ballet slippers. The price of the ballet slippers is 2/3rds of the price of the high heels. The high heels cost $60.\n",
            "\n",
            "\\[\n",
            "\\text{Cost of one pair of ballet slippers} = \\frac{2}{3} \\times 60 = 40 \\text{ dollars}\n",
            "\\]\n",
            "\n",
            "Next, let's find out the total cost for five pairs of ballet slippers:\n",
            "\n",
            "\\[\n",
            "\\text{Total cost for five pairs of ballet slippers} = 5 \\times 40 = 200 \\text{ dollars}\n",
            "\\]\n",
            "\n",
            "Now, add the cost of the high heels to the total cost for the ballet slippers to get the final amount Fern has to pay:\n",
            "\n",
            "\\[\n",
            "\\text{Total amount} = 60 + 200 = 260 \\text{ dollars}\n",
            "\\]\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "Fern pays a total of $260. </answer> \n",
            "Extracted:\n",
            "Fern pays a total of $260.\n",
            "-------------------- Question:\n",
            "A few friends are making a small movie.  They hire a few actors and that cost $1200.  They also get food for the 50 people there.  Each person gets $3 worth of food.  Equipment rental costs twice as much as food and actors combined.  They sold the movie for $10,000.  How much profit did they make? \n",
            "Answer:\n",
            "5950 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find the profit, we need to calculate the total expenses and then subtract them from the revenue. First, we'll determine the cost of food, then the cost of equipment rental, and finally the total expenses. After that, we'll subtract the total expenses from the movie's revenue to find the profit.\n",
            "\n",
            "The cost of food:\n",
            "- The food costs $3 per person for 50 people, so the total food cost is $3 * 50 = $150.\n",
            "\n",
            "Now, we calculate the equipment rental cost:\n",
            "- Equipment rental costs twice as much as food and actors combined.\n",
            "- The actors and food cost is $1200 + $150 = $1350.\n",
            "- So, equipment rental cost is 2 * $1350 = $2700.\n",
            "\n",
            "Now, we calculate the total expenses:\n",
            "- Total expenses = Cost of actors and food + Cost of equipment rental = $1350 + $2700 = $4050.\n",
            "\n",
            "Next, we find the profit:\n",
            "- Revenue from selling the movie = $10,000.\n",
            "- Profit = Revenue - Total Expenses = $10,000 - $4050 = $5950.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "They made a profit of $5950. </answer> \n",
            "Extracted:\n",
            "They made a profit of $5950.\n",
            "-------------------- Question:\n",
            "Out of the 200 apples in an orchard, 40 percent were rotten. Also, 70 percent of the rotten apples smelled. How many rotten apples in the orchard did not smell? \n",
            "Answer:\n",
            "24 \n",
            "Response:\n",
            "<reasoning>\n",
            "Given the number of apples in the orchard is 野00 apples, and 40 percent of these were rotten, we can calculate the total number of rotten apples as:\n",
            "\\[ \\text{Total Rotten Apples} = 40\\% \\times 200 = 0.40 \\times 200 = 80 \\]\n",
            "\n",
            "Out of the 80 rotten apples, 70 percent of them smelled. Hence, the number of rotten apples that smelled is:\n",
            "\\[ \\text{Rotten Apples Smelled} = 70\\% \\times 80 = 0.70 \\times 80 = 56 \\]\n",
            "\n",
            "To find the number of rotten apples that did not smell, we subtract the number of rotten apples that smelled from the total number of rotten apples:\n",
            "\\[ \\text{Rotten Apples Not Smelled} = \\text{Total Rotten Apples} - \\text{Rotten Apples Smelled} \\]\n",
            "\\[ \\text{Rotten Apples Not Smelled} = 80 - 56 = 24 \\]\n",
            "</reasoning>\n",
            "<answer>\n",
            "24 rotten apples in the orchard did not smell. </answer> \n",
            "Extracted:\n",
            "24 rotten apples in the orchard did not smell.\n",
            "-------------------- Question:\n",
            "Michael has 36 pets. 25% of them are dogs, 50% are cats, and the rest are bunnies. How many bunnies does he have? \n",
            "Answer:\n",
            "9 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find out how many bunnies Michael has, first calculate the number of dogs and cats, and then subtract those from the total number of pets. \n",
            "\n",
            "Given that 25% of his pets are dogs and 50% are cats, the percentage of bunnies is the remainder of the 100% - (25% + 50%). First, we calculate the number of dogs and cats:\n",
            "\n",
            "- The number of dogs = 25% of 36 = 0.25 * 36\n",
            "- The number of cats = 50% of 36 = 0.50 * 36\n",
            "\n",
            "The number of bunnies is then calculated as:\n",
            "\n",
            "- Number of bunnies = Total pets - Number of dogs - Number of cats\n",
            "\n",
            "<answer>\n",
            "Let's calculate the number of dogs and cats first:\n",
            "- Number of dogs = 0.25 * 36 = 9\n",
            "- Number of cats = 0.50 * 36 = 18\n",
            "\n",
            "So, the number of bunnies is:\n",
            "- Number of bunnies = 36 - 9 - 18 = 9\n",
            "\n",
            "Therefore, Michael has 9 bunnies.\n",
            "</answer>\n",
            "<reasoning>\n",
            "To find out how many bunnies Michael has, first calculate the number of dogs and cats, and then subtract those from the total number of pets.\n",
            "\n",
            "Given that 25% of his pets are dogs and 50% are cats, the percentage of bunnies is the remainder of the 100% - (25% + 50%). First, we calculate the number of dogs and cats:\n",
            "\n",
            "- The number of dogs = 25% of 36 = 0.25 * 36 = 9\n",
            "- The number of cats = 50% of 36 = 0.50 * 36 = 18\n",
            "\n",
            "The number of bunnies is then calculated as:\n",
            "\n",
            "- Number of bunnies = Total pets - Number of dogs - Number of cats\n",
            "- Number of bunnies = 36 - 9 - 18 = 9\n",
            "\n",
            "Therefore, Michael has 9 bunnies.\n",
            "</reasoning>\n",
            "<answer>9</answer> \n",
            "Extracted:\n",
            "9\n",
            "-------------------- Question:\n",
            "Johnny is a dog walker. He can walk 3 dogs at once. He gets paid $15 for a 30-minute walk and $20 for a 60-minute walk. Johnny works for 4 hours per day. If he always walks the maximum number of dogs possible and 6 dogs have 60-minute walks per day, how much money does he make in a week where he works 5 days? \n",
            "Answer:\n",
            "1500 \n",
            "Response:\n",
            "<reasoning>\n",
            "Johnny works for 4 hours per day, which is equivalent to 240 minutes. If he always walks the maximum number of dogs possible and he has 6 dogs with 60-minute walks, we need to determine if Johnny can accommodate this schedule considering he can walk 3 dogs at once. \n",
            "\n",
            "First, let’s determine the number of 60-minute walks Johnny can accommodate in a day by dividing the total working time (240 minutes) by the duration of each 60-minute walk (60 minutes). \n",
            "\\[\n",
            "\\frac{240 \\text{ minutes}}{60 \\text{ minutes per walk}} = 4 \\text{ walks}\n",
            "\\]\n",
            "So, Johnny can do 4 walks of 60 minutes each in a day. Since he has 6 dogs needing 60-minute walks, it's not possible for him to always fit all dogs with 60-minute walks in a day because he only completes 4 full 60-minute walks per day. Therefore, he will have 2 dogs waiting for their next walks, which could be either 30-minute walks or another 60-minute walk.\n",
            "\n",
            "Since he has 4 walks to do and he needs to manage 6 dogs with 60-minute walks, he can walk the 4 dogs with 60-minute walks and the remaining 2 dogs with 30-minute walks. \n",
            "\n",
            "For each 60-minute walk:\n",
            "\\[\n",
            "4 \\text{ walks} \\times 20 \\text{ dollars per walk} = 80 \\text{ dollars per day}\n",
            "\\]\n",
            "\n",
            "For each 30-minute walk:\n",
            "\\[\n",
            "2 \\text{ walks} \\times 15 \\text{ dollars per walk} = 30 \\text{ dollars per day}\n",
            "\\]\n",
            "\n",
            "The total daily earnings are then:\n",
            "\\[\n",
            "80 \\text{ dollars} + 30 \\text{ dollars} = 110 \\text{ dollars per day}\n",
            "\\]\n",
            "\n",
            "In a week (5 days), Johnny's earnings will be:\n",
            "\\[\n",
            "5 \\text{ days} \\times 110 \\text{ dollars per day} = 550 \\text{ dollars per week}\n",
            "\\]\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "Johnny makes $550 in a week. \n",
            "Extracted:\n",
            "Johnny makes $550 in a week.\n",
            "-------------------- Question:\n",
            "Giselle had 20 female doves in a birdhouse. After a month, the doves laid 3 eggs each. If 3/4 of the eggs hatched, calculate the total number of doves Giselle has now. \n",
            "Answer:\n",
            "65 \n",
            "Response:\n",
            "<reasoning>\n",
            "Giselle initially had 20 female doves. Each dove laid 3 eggs, so the total number of eggs laid is 20 * 3 = 60 eggs. If 3/4 of the eggs hatched, the number of eggs that hatched is 60 * 3/4 = 45. Since each hatched egg becomes one dove, the number of new doves that hatched is 45. Adding these new doves to the original number of female doves, the total number of doves now is 20 + 45 = 65.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Giselle now has 65 doves.\n",
            "</answer> \n",
            "Extracted:\n",
            "Giselle now has 65 doves.\n",
            "-------------------- Question:\n",
            "John buys 2 packs of gum and 3 candy bars.  Each stick of gum cost half as much as the candy bar.  If the candy bar cost $1.5 each, how much did he pay in total? \n",
            "Answer:\n",
            "6 \n",
            "Response:\n",
            "<reasoning>\n",
            "- John buys 2 packs of gum and 3 candy bars\n",
            "- The cost of each candy bar is given as $1.5\n",
            "- The cost of each stick of gum is half the cost of a candy bar, so each stick of gum costs $1.5 / 2 = $0.75\n",
            "- The total cost for the candy bars is 3 * $1.5 = $4.5\n",
            "- The total cost for the gum is 2 * $0.75 = $1.5\n",
            "- The total amount spent is the sum of the cost of the candy bars and the gum, which is $4.5 + $1.5 = $6.0\n",
            "\n",
            "Thus, John paid a total of $6.0.\n",
            "</reasoning>\n",
            "<answer>\n",
            "John paid a total of $6.0.\n",
            "</answer> \n",
            "Extracted:\n",
            "John paid a total of $6.0.\n",
            "-------------------- Question:\n",
            "Sarah's age is equal to three times Mark's age minus 4. Mark is four years older than Billy. Billy is half Ana's age. If Ana will be 15 in 3 years, how old is Sarah? \n",
            "Answer:\n",
            "26 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find out Sarah's age, we need to follow the clues given and solve step by step.\n",
            "\n",
            "First, from Ana's age deduction: \n",
            "- Ana will be 15 in 3 years. This means Ana's current age is \\(15 - 3 = 12\\).\n",
            "\n",
            "Next, using Billy's age which is defined in terms of Ana's age:\n",
            "- Billy is half of Ana's age, so Billy's age is \\(12 / 2 = 6\\).\n",
            "\n",
            "Now, using Mark's age which is defined in terms of Billy's age:\n",
            "- Mark is four years older than Billy, so Mark's age is \\(6 + 4 = 10\\).\n",
            "\n",
            "Finally, we can determine Sarah's age using her age in terms of Mark's age:\n",
            "- Sarah's age is three times Mark's age minus 4: \\(3 \\times 10 - 4 = 30 - 4 = 26\\).\n",
            "\n",
            "Therefore, Sarah is 26 years old.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "26\n",
            "</answer> \n",
            "Extracted:\n",
            "26\n",
            "-------------------- Question:\n",
            "Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years? \n",
            "Answer:\n",
            "3982 \n",
            "Response:\n",
            "<reasoning>\n",
            "To solve this problem, we'll need to calculate the total amount accumulated over the two-year period considering both the initial investment and the monthly deposits. We'll use the formula for the future value of a series and periodic investments:\n",
            "\n",
            "1. **Initial Investment Principal**: $1000\n",
            "2. **Annual Interest Rate**: 15.37% (10% + 10% monthly compounded)\n",
            "3. **Monthly Deposit**: $100\n",
            "4. **Duration**: 2 years (24 months)\n",
            "\n",
            "The formula for the future value of a series of regular monthly deposits into a savings account with a periodic interest rate is given by:\n",
            "\n",
            "\\[ FV = P \\times \\left( \\frac{(1 + r)^n - 1} {r} \\right) + (P \\times r) \\times \\left( \\frac{((1 + r)^n - (1 + r)}{ (1 + r) - 1}) \\right) \\]\n",
            "\n",
            "Where:\n",
            "- \\( FV \\) is the future value.\n",
            "- \\( P \\) is the monthly deposit.\n",
            "- \\( r \\) is the monthly interest rate.\n",
            "- \\( n \\) is the total number of deposits.\n",
            "\n",
            "In this case:\n",
            "- \\( P = \\$100 \\)\n",
            "- Annual interest rate \\( = 10\\% = 0.10 \\)\n",
            "- Monthly interest rate \\( = 0.10 / 12 = 0.008333 \\) (since the interest is compounded monthly)\n",
            "- \\( n = 24 \\) months over 2 years\n",
            "\n",
            "Let's calculate the future value using the formula. We will also consider the periodic interest and adjust the total calculations accordingly.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Based on the calculation, the future value of Joseph's investment after two years, considering his $100 monthly deposits and a yearly interest rate of 10% compounded monthly, is approximately $4214.54. Therefore, Joseph will have approximately $4214.54 in the fund after two years.\n",
            "</answer> \n",
            "Extracted:\n",
            "Based on the calculation, the future value of Joseph's investment after two years, considering his $100 monthly deposits and a yearly interest rate of 10% compounded monthly, is approximately $4214.54. Therefore, Joseph will have approximately $4214.54 in the fund after two years.\n",
            "-------------------- Question:\n",
            "Billy is reducing raspberry juice down to make syrup. He reduces 6 quarts of juice to 1/12 of its original volume, then adds 1 cup of sugar. What is the final volume of the syrup in cups? (There are 4 cups in a quart) \n",
            "Answer:\n",
            "3 \n",
            "Response:\n",
            "<reasoning>\n",
            "First, let's convert the initial quantity of juice into cups. Since there are 4 cups in a quart and Billy uses 6 quarts of juice, the total volume in cups is \\(6 \\times 4 = 24\\) cups.\n",
            "\n",
            "Next, Billy reduces the juice to \\(\\frac{1}{12}\\) of its original volume. So, the volume after reduction is \\(24 \\times \\frac{1}{12} = 2\\) cups.\n",
            "\n",
            "Then, he adds 1 cup of sugar, but since the question is only asking for the final volume of the syrup in cups and assuming that the sugar does not change the volume (it's a different phase), the final volume remains 2 cups.\n",
            "\n",
            "Finally, since we need to provide the answer in cups, we conclude that the final volume of the syrup in cups is 2.\n",
            "</reasoning>\n",
            "<answer>\n",
            "2</answer> \n",
            "Extracted:\n",
            "2\n",
            "-------------------- Question:\n",
            "There were 40 kids on Lake Pleasant. A fourth of the kids went tubing, but only half of the tubers went rafting. How many of the kids who joined the rafting excursion were also on the tubing excursion? \n",
            "Answer:\n",
            "5 \n",
            "Response:\n",
            "<reasoning>\n",
            "We know that there were 40 kids on Lake Pleasant. A fourth of the kids went tubing. First, we calculate the number of kids who went tubing:\n",
            "\n",
            "\\[\n",
            "\\frac{1}{4} \\times 40 = 10 \\text{ kids}\n",
            "\\]\n",
            "\n",
            "Next, we need to find out how many of these tubing kids also went rafting. We are told that only half of the tubers went rafting. So, we calculate:\n",
            "\n",
            "\\[\n",
            "\\frac{1}{2} \\times 10 = 5 \\text{ kids}\n",
            "\\]\n",
            "\n",
            "Thus, there were 5 kids who were on both the tubing and rafting excursions.\n",
            "</reasoning>\n",
            "<answer>\n",
            "5 kids were on both the tubing and rafting excursions.\n",
            "</answer> \n",
            "Extracted:\n",
            "5 kids were on both the tubing and rafting excursions.\n",
            "-------------------- Question:\n",
            "James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end? \n",
            "Answer:\n",
            "245 \n",
            "Response:\n",
            "<reasoning>\n",
            "James starts with 500 Legos. He uses half of them to build a castle, so he uses \\( \\frac{500}{2} = 250 \\) Legos. This leaves him with \\( 500 - 250 = 250 \\) Legos that he didn't use. Out of these leftover Legos, he found 45 (Since 5 are missing). This means there are \\( 250 - 45 = 205 \\) Legos in the box at the end.\n",
            "</reasoning>\n",
            "<answer>\n",
            "205\n",
            "</answer> \n",
            "Extracted:\n",
            "205\n",
            "-------------------- Question:\n",
            "Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average? \n",
            "Answer:\n",
            "160 \n",
            "Response:\n",
            "<reasoning>\n",
            "To determine the average number of pages per book, we first need to determine the total number of pages in Jack's stack of books. Since we know that each inch of thickness corresponds to 80 pages, and the stack of books is 12 inches thick, the total number of pages can be calculated as follows:\n",
            "\\[ \\text{Total pages} = 12\\ \\text{inches} \\times 80\\ \\text{pages per inch} \\]\n",
            "\n",
            "Next, we need to find the average number of pages per book. Since Jack has 6 books, the average number of pages per book is:\n",
            "\\[ \\text{Average pages per book} = \\frac{\\text{Total pages}}{6} \\]\n",
            "\n",
            "We can calculate the total number of pages first and then find the average number of pages per book.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Total pages in the stack of books:\n",
            "\\[ 12\\ \\text{inches} \\times 80\\ \\text{pages per inch} = 960\\ \\text{pages} \\]\n",
            "\n",
            "Average number of pages per book:\n",
            "\\[ \\frac{960\\ \\text{pages}}{6\\ \\text{books}} = 160\\ \\text{pages per book} \\]\n",
            "\n",
            "Therefore, each book on average contains 160 pages. \n",
            "</answer> \n",
            "Extracted:\n",
            "Total pages in the stack of books:\n",
            "\\[ 12\\ \\text{inches} \\times 80\\ \\text{pages per inch} = 960\\ \\text{pages} \\]\n",
            "\n",
            "Average number of pages per book:\n",
            "\\[ \\frac{960\\ \\text{pages}}{6\\ \\text{books}} = 160\\ \\text{pages per book} \\]\n",
            "\n",
            "Therefore, each book on average contains 160 pages.\n",
            "-------------------- Question:\n",
            "A fox can run at the maximum speed of 50 kilometers per hour. Considering the fox would run at a constant speed, what distance would he make during 120 minutes? \n",
            "Answer:\n",
            "100 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find out the distance the fox can make during 120 minutes, we first need to convert the time from minutes to hours since the speed is given in kilometers per hour. There are 60 minutes in an hour, so 120 minutes is equivalent to 120 / 60 = 2 hours. \n",
            "\n",
            "Next, we use the formula for distance which is:\n",
            "\\[ \\text{Distance} = \\text{Speed} \\times \\text{Time} \\]\n",
            "\n",
            "Given the speed of the fox is 50 kilometers per hour and the time is 2 hours, we can calculate the distance.\n",
            "\\[ \\text{Distance} = 50 \\, \\text{km/h} \\times 2 \\, \\text{h} = 100 \\, \\text{km} \\]\n",
            "\n",
            "Therefore, the fox would make a distance of 100 kilometers in 120 minutes.\n",
            "</reasoning>\n",
            "<answer>\n",
            "The fox would make a distance of 100 kilometers in 120 minutes.\n",
            "</answer> \n",
            "Extracted:\n",
            "The fox would make a distance of 100 kilometers in 120 minutes.\n",
            "-------------------- Question:\n",
            "Aryan owes Fernanda $1200, which is twice what Kyro owes Fernanda. Aryan decides to pay 60% of her debt to Fernanda, and Kyro pays Fernanda 80% of her dept. If Fernanda had $300 in her savings account and adds the money she's been paid by her debtors to the savings account, calculate the total amount of money in her savings account now. \n",
            "Answer:\n",
            "1500 \n",
            "Response:\n",
            "<reasoning>\n",
            "Let's first calculate the amount Kyro owes Fernanda. Since Aryan owes twice what Kyro owes, we have:\n",
            "\\[ \\text{Kyro's debt} = \\frac{\\text{Aryan's debt}}{2} = \\frac{1200}{2} = 600 \\]\n",
            "\n",
            "Next, let's determine how much Aryan and Kyro each pay Fernanda:\n",
            "\\[ \\text{Aryan's payment} = 0.60 \\times \\text{Aryan's debt} = 0.60 \\times 1200 = 720 \\]\n",
            "\\[ \\text{Kyro's payment} = 0.80 \\times \\text{Kyro's debt} = 0.80 \\times 600 = 480 \\]\n",
            "\n",
            "Now we add the payments made by Aryan and Kyro to Fernanda's savings account:\n",
            "\\[ \\text{Total received} = \\text{Aryan's payment} + \\text{Kyro's payment} = 720 + 480 = 1200 \\]\n",
            "\n",
            "Fernanda starts with $300 in her savings account. Adding the $1200 from her debtors to her savings:\n",
            "\\[ \\text{Total in savings} = 300 + 1200 = 1500 \\]\n",
            "\n",
            "Therefore, the total amount of money in Fernanda's savings account now is:\n",
            "\\[ \\text{Total in savings} = 1500 \\]\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "1500\n",
            "</answer> \n",
            "Extracted:\n",
            "1500\n",
            "-------------------- Question:\n",
            "John buys cans of soup for buy 1 get one free.  He gets 30 cans with a normal price of $0.60.  How much does he pay? \n",
            "Answer:\n",
            "9 \n",
            "Response:\n",
            "<reasoning>\n",
            "John gets 30 cans of soup but the offer is \"buy 1 get 1 free, \" meaning for every 2 cans he only pays for 1 can. Therefore, with 30 cans, he only needs to pay for half of them. Half of 30 is 15 cans. Given that each can costs $0.60, we can calculate the total cost by multiplying the number of cans he pays for by the price per can.\n",
            "</reasoning>\n",
            "<answer>\n",
            "The total amount John pays is 15 cans * $0.60/can = $9.00.\n",
            "</answer> \n",
            "Extracted:\n",
            "The total amount John pays is 15 cans * $0.60/can = $9.00.\n",
            "-------------------- Question:\n",
            "Cristine bought a dozen lemons and gave 1/4 of them to her neighbor. How many lemons does Cristine have left? \n",
            "Answer:\n",
            "9 \n",
            "Response:\n",
            "<reasoning>\n",
            "Cristine initially bought a dozen lemons. Since one dozen equals 12, she had 12 lemons at the start. She gave away 1/4 of them to her neighbor. To find out how many lemons she gave away, we calculate 1/4 of 12, which is 12 * (1/4) = 3 lemons. To find out how many lemons Cristine has left, we subtract the 3 lemons she gave away from the initial 12 lemons, which is 12 - 3 = 9 lemons.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Cristine has 9 lemons left.\n",
            "</answer> \n",
            "Extracted:\n",
            "Cristine has 9 lemons left.\n",
            "-------------------- Question:\n",
            "Eric has 4 ninja throwing stars. His friend Chad has twice as many. Then Jeff bought 2 ninja stars from Chad. Jeff now has 6 throwing stars. How many ninja throwing stars do they have altogether? \n",
            "Answer:\n",
            "16 \n",
            "Response:\n",
            "<reasoning>\n",
            "Let's start by calculating the initial number of ninja throwing stars Eric and Chad have. Eric has 4 throwing stars. Chad has twice as many as Eric, so Chad has 4 * 2 = 8 throwing stars. After Jeff buys 2 throwing stars from Chad, Chad has 8 - 2 = 6 throwing stars left. We are given that after buying the stars, Jeff has 6 throwing stars. Since Jeff bought 2 throwing stars, he originally had 6 - 2 = 4 throwing stars. Now, let's add up all the throwing stars together:\n",
            "\n",
            "- Eric's throwing stars: 4\n",
            "- Chad's throwing stars (after selling 2): 6\n",
            "- Jeff's throwing stars (after buying 2): 6\n",
            "\n",
            "Altogether, they have 4 (Eric) + 6 (Chad) + 6 (Jeff) = 16 throwing stars.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "16 throwing stars</answer> \n",
            "Extracted:\n",
            "16 throwing stars\n",
            "-------------------- Question:\n",
            "Remi wants to drink more water. He has a refillable water bottle that holds 20 ounces of water. That week Remi refills the bottle 3 times a day and drinks the whole bottle each time except for twice when he accidentally spills 5 ounces the first time and 8 ounces the second time. In 7 days how many ounces of water does Remi drink? \n",
            "Answer:\n",
            "407 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find out how many ounces of water Remi drinks in a week, let's break down the problem step by step:\n",
            "\n",
            "1. **Water Capacity and Refills**:\n",
            "   - A full refillable water bottle holds 20 ounces.\n",
            "   - Remi refills it 3 times a day.\n",
            "   - Water intake per day = 20 ounces * 3 refills = 60 ounces.\n",
            "\n",
            "2. **Adjustments for Spills**:\n",
            "   - Remi spills 5 ounces the first time he spills and 8 ounces the second time.\n",
            "   - Adjust drainage for a day: \n",
            "      - If Remi spills 5 ounces once, he still drinks 20 ounces - 5 ounces = 15 ounces.\n",
            "      - If he spills 8 ounces once, he still drinks 20 ounces - 8 ounces = 12 ounces.\n",
            "   \n",
            "3. **Regular Daily Consumption**:\n",
            "   - Normal water intake per day: 60 ounces.\n",
            "\n",
            "4. **Total Consumption**:\n",
            "   - Water intake for 7 days would be 60 ounces * 7 days = 420 ounces, assuming no spills.\n",
            "   - When spills are considered:\n",
            "     - For the first spill, Remi drinks 60 ounces - 5 ounces = 55 ounces.\n",
            "     - For the second spill, Remi drinks 60 ounces - 8 ounces = 52 ounces.\n",
            "     - For the next 5 days (except the specific days that spill):\n",
            "       - He drinks 60 ounces/day each day, which amounts to 60 ounces * 5 days = 300 ounces.\n",
            "     - Summing up the consumption:\n",
            "       - First day with spill: 55 ounces\n",
            "       - Next 5 days: 300 ounces\n",
            "       - Total: 55 ounces + 300 ounces = 355 ounces\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>Remi drinks a total of 355 ounces of water in 7 days considering his spills.</answer> \n",
            "Extracted:\n",
            "Remi drinks a total of 355 ounces of water in 7 days considering his spills.\n",
            "-------------------- Question:\n",
            "Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill? \n",
            "Answer:\n",
            "2 \n",
            "Response:\n",
            "<reasoning>\n",
            "To determine how long it takes Tom to climb up the hill, we first need to understand Tom's climbing time in comparison to Elizabeth's. Elizabeth takes multipart min to climb the hill, which is 30 minutes.\n",
            "\n",
            "Tom takes four times as long as Elizabeth, so we multiply Elizabeth's time by 4 to find Tom's time.\n",
            "\\[ 30 \\text{ minutes} \\times 4 = 120 \\text{ minutes} \\]\n",
            "\n",
            "Since the question asks for the time in hours, we convert minutes to hours. There are 60 minutes in an hour, so we divide Tom's time in minutes by 60.\n",
            "\\[ 120 \\text{ minutes} \\div 60 = 2 \\text{ hours} \\]\n",
            "\n",
            "Therefore, it takes Tom 2 hours to climb up the hill.\n",
            "</reasoning>\n",
            "<answer>\n",
            "2 hours\n",
            "</answer> \n",
            "Extracted:\n",
            "2 hours\n",
            "-------------------- Question:\n",
            "For his birthday, Geoffrey received clothes and money. His grandmother gave him €20, his aunt €25 and his uncle €30. With what he already had, he now has €125 in his wallet. He goes to a video game store and gets 3 games that cost 35 euros each. How much does he have left after this purchase? \n",
            "Answer:\n",
            "20 \n",
            "Response:\n",
            "<reasoning>\n",
            "First, we calculate the total amount of money Geoffrey received from his family members. Adding these amounts together: €20 + €25 + €牡nce = €70. Next, we add the money he already had to the €70 he received: €125 = €195. Then, Geoffrey buys 3 video games that cost €35 each, so the total cost of the games is 3 × €35 = €105. Finally, to find out how much money Geoffrey has left after the purchase, we subtract the amount spent on the games from the amount he had after receiving money from his family: €195 - €105 = €90.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Geoffrey has €90 left after his purchase.\n",
            "</answer> \n",
            "Extracted:\n",
            "Geoffrey has €90 left after his purchase.\n",
            "-------------------- Question:\n",
            "The teacher divided the students into four groups. One group had 5 students, another 8 students, and the third 7 students. If there were 24 total students, how many students were in the fourth group? \n",
            "Answer:\n",
            "4 \n",
            "Response:\n",
            "<reasoning>\n",
            "Let's denote the number of students in the fourth group as \\( x \\). According cavars of the given groups, we have the following information:\n",
            "- One group has 5 students.\n",
            "- Another group has 8 students.\n",
            "- The third group has 7 students.\n",
            "The fourth group's students are to be found. Given that in total, there were 24 students, we can set up the following equation to solve for \\( x \\):\n",
            "\\[ 5 + 8 + 7 + x = 24 \\]\n",
            "We will find the value of \\( x \\) by simplifying the left-hand side and solving for \\( x \\).\n",
            "</reasoning>\n",
            "<answer>\n",
            "5 + 8 + 7 = 20\n",
            "Therefore, \\( x = 24 - 20 = 4 \\).\n",
            "The number of students in the fourth group is 4.\n",
            "</answer> \n",
            "Extracted:\n",
            "5 + 8 + 7 = 20\n",
            "Therefore, \\( x = 24 - 20 = 4 \\).\n",
            "The number of students in the fourth group is 4.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 104b68c5-c33c-4265-a936-c66a4f92e4f3)') - silently ignoring the lookup for the file config.json in unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "TreShawn's mom needs to order pizza for her son and his two friends. She knows that TreShawn always eats 1/2 a pizza. His friend Michael eats 1/3 of a pizza and his other friend LaMar eats 1/6 of a pizza. How many pizzas does she need to order? \n",
            "Answer:\n",
            "1 \n",
            "Response:\n",
            "<reasoning>\n",
            "TreShawn's mom needs to order enough pizza to satisfy TreShawn, Michael, and LaMar. To determine how many pizzas she needs to order, we should first calculate the total amount of pizza they eat together. TreShawn eats 1/2 a pizza, Michael eats 1/3 of a pizza, and LaMar eats 1/6 of a pizza. Adding these fractions together will give us the total amount of pizza needed.\n",
            "\n",
            "To add the fractions, we need a common denominator. The denominators are 2, 3, and 6. The smallest number that 2, 3, and 6 all divide into without leaving remainders is 6 (the least common multiple of 2, 3, and 6). We convert the fractions:\n",
            "- TreShawn's portion: 1/2 = 3/6 (since \\(1/2 = 3/6\\) when both numerator and denominator are multiplied by 3)\n",
            "- Michael's portion: 1/3 = 2/6 (since \\(1/3 = 2/6\\) when both numerator and denominator are multiplied by 2)\n",
            "- LaMar's portion: 1/6 (already in a common denominator)\n",
            "\n",
            "Adding these together: 3/6 + 2/6 + 1/6 = 6/6 = 1. This means the total amount of pizza needed is equal to one whole pizza.\n",
            "\n",
            "Therefore, TreShawn's mom needs to order 1 pizza.\n",
            "</reasoning>\n",
            "<answer>\n",
            "TreShawn's mom needs to order 1 pizza.\n",
            "</answer> \n",
            "Extracted:\n",
            "TreShawn's mom needs to order 1 pizza.\n",
            "-------------------- Question:\n",
            "My brother and I have thirty minutes to go to school, without being late. It takes us 15 minutes to arrive at the school gate, and another 6 minutes to get to the school building. How much time do we have to get to our room, without being late? \n",
            "Answer:\n",
            "9 \n",
            "Response:\n",
            "<reasoning>\n",
            "We need to determine the time remaining for getting to the room after accounting for the total time spent on arriving at the school gate and getting to the school building. We know the total time to go to school without being late is 30 minutes. It takes us 15 minutes to reach the school gate and an additional 6 minutes to reach the school building. Therefore, the combined time spent on these activities is the sum of these two times. Once we subtract this time from the total time to arrive without being late, the remaining time will be the time we have to get to our room.\n",
            "\n",
            "First, let's find the total time spent:\n",
            "\\[ \\text{Total time spent} = \\text{Time to the school gate} + \\text{Time to the school building} \\]\n",
            "\\[ \\text{Total time spent} = 15 \\, \\text{minutes} + 6 \\, \\text{minutes} = 21 \\, \\text{minutes} \\]\n",
            "\n",
            "Next, we subtract the total time spent from the total time to arrive on time:\n",
            "\\[ \\text{Time to get to the room} = \\text{Total time to arrive on time} - \\text{Total time spent}\n",
            "\\]\n",
            "\\[ \\text{Time to get to the room} = 30 \\, \\text{minutes} - 21 \\, \\text{minutes} = 9 \\, \\text{minutes} \\]\n",
            "\n",
            "So, the time we have to get to our room, without being late, is 9 minutes.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "9 minutes\n",
            "</answer> \n",
            "Extracted:\n",
            "9 minutes\n",
            "-------------------- Question:\n",
            "At a convention, 16 of 36 delegates arrived with pre-printed name badges. Half of the remaining delegates made their own, hand-written name badges. How many delegates were not wearing name badges? \n",
            "Answer:\n",
            "10 \n",
            "Response:\n",
            "<reasoning>\n",
            "We start by determining the number of delegates who were not wearing name badges. Initially, there were 36 delegates at the convention. According大大小小的说，16 delegates arrived with pre-printed name badges. Therefore, the number of delegates not wearing pre-printed badges initially is:\n",
            "\\[ 36 - 16 = 20 \\text{ delegates} \\]\n",
            "\n",
            "Then, half of the remaining delegates (those without pre-printed badges) made their own hand-written name badges. So we need to find half of 20:\n",
            "\\[ \\frac{20}{2} = 10 \\text{ delegates} \\]\n",
            "\n",
            "This means after the remaining half made hand-written badges, they would now have badges, and thus:\n",
            "\\[ 20 - 10 = 10 \\text{ delegates} \\]\n",
            "\n",
            "They did not have badges initially, but the half who made them hand-written would now have badges. So we need to add the 10 delegates who made their own hand-written badges to the 16 who arrived with pre-printed badges to find the total number of delegates wearing badges:\n",
            "\\[ 16 + 10 = 26 \\]\n",
            "\n",
            "Therefore, the number of delegates not wearing any badges is:\n",
            "\\[ 36 - 26 = 10 \\text{ delegates} \\]\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "10\n",
            "</answer> \n",
            "Extracted:\n",
            "10\n",
            "-------------------- Question:\n",
            "Hakeem has always loved artichoke dip and plans to make it this weekend. He has $15 dollars to buy artichokes and can find the rest of the ingredients at home. It takes 3 artichokes to make 5 ounces of dip. How many ounces can he make if artichokes cost $1.25 each? \n",
            "Answer:\n",
            "20 \n",
            "Response:\n",
            "<reasoning>\n",
            "Hakeem wants to make artichoke dip but first needs to determine how many artichokes he can buy with $15. Given that each artichoke costs $1.2小女孩儿25, we can calculate how many artichokes he can buy by dividing $15 by $1.25. Then, knowing that 3 artichokes make 5 ounces of dip, we can find out how many ounces he can make with the number of artichokes he purchases.\n",
            "\n",
            "First, let's calculate the number of artichokes Hakeem can buy:\n",
            "\\[ \\text{Artichokes bought} = \\frac{\\text{Total money}}{\\text{Price per artichoke}} = \\frac{15}{1.25} \\]\n",
            "\n",
            "Next, we will determine the total amount of artichoke dip he can make based on the number of artichokes he bought. The conversion factor from artichokes to ounces of dip is:\n",
            "\\[ \\text{Ounces of dip} = \\text{Artichokes bought} \\times \\left( \\frac{5 \\text{ ounces of dip}}{3 \\text{ artichokes}} \\right) \\]\n",
            "\n",
            "Let's do the calculation:\n",
            "</reasoning>\n",
            "<answer>\n",
            "First, we calculate the number of artichokes Hakeem can buy:\n",
            "\\[ \\text{Artichokes bought} = \\frac{15}{1.25} = 12 \\text{ artichokes} \\]\n",
            "\n",
            "Now, using the conversion factor, we calculate the total amount of artichoke dip Hakeem can make:\n",
            "\\[ \\text{Ounces of dip} = 12 \\times \\left( \\frac{5}{3} \\right) = 12 \\times 1.6667 = 20 \\text{ ounces of dip} \\]\n",
            "\n",
            "Therefore, Hakeem can make 20 ounces of artichoke dip.\n",
            "</answer>\n",
            " \n",
            "Extracted:\n",
            "First, we calculate the number of artichokes Hakeem can buy:\n",
            "\\[ \\text{Artichokes bought} = \\frac{15}{1.25} = 12 \\text{ artichokes} \\]\n",
            "\n",
            "Now, using the conversion factor, we calculate the total amount of artichoke dip Hakeem can make:\n",
            "\\[ \\text{Ounces of dip} = 12 \\times \\left( \\frac{5}{3} \\right) = 12 \\times 1.6667 = 20 \\text{ ounces of dip} \\]\n",
            "\n",
            "Therefore, Hakeem can make 20 ounces of artichoke dip.\n",
            "-------------------- Question:\n",
            "Mason is mixing up some salad dressing. He takes a bowl that holds 150 ml and fills it 2/3rds with oil and 1/3rd with vinegar. The oil weighs 5 g/ml and the vinegar weighs 4 g/ml. How many grams does the salad dressing weigh? \n",
            "Answer:\n",
            "700 \n",
            "Response:\n",
            "<reasoning>\n",
            "Mason fills a bowl that holds 150 ml. The bowl is 2/3 filled with oil and 1/3 filled with vinegar. To determine the weight of the salad dressing, we first need to find out how much volume of oil and vinegar he has added.\n",
            "\n",
            "The volume of oil:\n",
            "\\[\n",
            "\\text{Volume of oil} = \\frac{2}{3} \\times 150 \\, \\text{ml} = 100 \\, \\text{ml}\n",
            "\\]\n",
            "\n",
            "The volume of vinegar:\n",
            "\\[\n",
            "\\text{Volume of vinegar} = \\frac{1}{3} \\times 150 \\, \\text{ml} = 50 \\, \\text{ml}\n",
            "\\]\n",
            "\n",
            "Now let's find out the weight of the oil and vinegar. \n",
            "\n",
            "The weight of the oil:\n",
            "\\[\n",
            "\\text{Weight of oil} = 100 \\, \\text{ml} \\times 5 \\, \\text{g/ml} = 500 \\, \\text{g}\n",
            "\\]\n",
            "\n",
            "The weight of the vinegar:\n",
            "\\[\n",
            "\\text{Weight of vinegar} = 50 \\, \\text{ml} \\times 4 \\, \\text{g/ml} = 200 \\, \\text{g}\n",
            "\\]\n",
            "\n",
            "The total weight of the salad dressing:\n",
            "\\[\n",
            "\\text{Total weight} = \\text{Weight of oil} + \\text{Weight of vinegar} = 500 \\, \\text{g} + 200 \\, \\text{g} = 700 \\, \\text{g}\n",
            "\\]\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "The salad dressing weighs 700 grams. </answer> \n",
            "Extracted:\n",
            "The salad dressing weighs 700 grams.\n",
            "-------------------- Question:\n",
            "Jenny makes and freezes pans of lasagna all week so she can sell them at the market on the weekend.  It costs Jenny $10.00 in ingredients to make 1 pan of lasagna.  If she makes and sells 20 pans over the weekend at $25.00 apiece, how much does she make after factoring in expenses? \n",
            "Answer:\n",
            "300 \n",
            "Response:\n",
            "<reasoning>\n",
            "Jenny's total expenses for 20 pans of lasagna can be calculated by multiplying the cost per pan by the total number of pans she makes. The cost per pan is $10.00 and she makes 20 pans, so her total expenses are $10.00 * 20 = $200.00. Jenny sells each pan for $25.00, and she sells 20 pans, so her total sales are $25.00 * 20 = $500.00. Her profit is the total sales minus the total expenses, which is $500.00 - $200.00 = $300.00.\n",
            "</reasoning>\n",
            "<answer>\n",
            "Jenny makes a profit of $300.00 after factoring in expenses.\n",
            "</answer> \n",
            "Extracted:\n",
            "Jenny makes a profit of $300.00 after factoring in expenses.\n",
            "-------------------- Question:\n",
            "The school decided to add 20% to the gym budget. In a normal month, they could buy 15 dodgeballs for $5 each if they spent the entire budget on that. If they only buy softballs instead, which cost $9 each, how many can they buy with the new budget? \n",
            "Answer:\n",
            "10 \n",
            "Response:\n",
            "<reasoning>\n",
            "The school's initial budget for the gym can be determined by the cost of buying 15 dodgeballs at $5 each. This gives us an initial budget of \\( 15 \\times 5 = 75 \\) dollars. With a 20% increase in the budget, the new budget is \\( 75 \\times 1.20 = 90 \\) dollars. At this new budget, we can then determine how many softballs of $9 each can be bought. \n",
            "\n",
            "The maximum number of softballs they can now purchase with the increased budget is \\( \\frac{90}{9} = 10 \\).\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "With the new budget, they can buy 10 softballs. </answer> \n",
            "Extracted:\n",
            "With the new budget, they can buy 10 softballs.\n",
            "-------------------- Question:\n",
            "Each dandelion produces 300 seeds. 1/3rd of the seeds land in water and die. 1/6 of the starting number are eaten by insects. Half the remainder sprout and are immediately eaten. How many dandelions survive long enough to flower? \n",
            "Answer:\n",
            "75 \n",
            "Response:\n",
            "<reasoning>\n",
            "To find the number of dandelions that survive long enough to flower, we can follow these steps:\n",
            "\n",
            "1. Start with the total number of seeds, which is 300.\n",
            "2. Calculate how many seeds land in water and die (1/3 of the total).\n",
            "3. Subtract the number of seeds that die in the water from step 2 from the original total to get the number of remaining seeds.\n",
            "4. Calculate how many of these remaining seeds are eaten by insects (1/6 of the remaining seeds).\n",
            "5. Subtract the number of seeds eaten by insects from step 4 from the remaining seeds after step 3 to get the number of seeds that survive up to this point.\n",
            "6. Half of these seeds sprout and are immediately eaten.\n",
            "7. Calculate the number of seeds that survive the second phase (half of the remaining seeds after step 5).\n",
            "8. The number of surviving seeds from step 7 is the answer to the question.\n",
            "\n",
            "Let's do the calculations:\n",
            "1. Total number of seeds: 300.\n",
            "2. Seeds that land in water and die: 300 * 1/3 = 100.\n",
            "3. Remaining seeds after step 2: 300 - 100 = 200.\n",
            "4. Seeds eaten by insects: 200 * 1/6 = 100/3 ≈ 33.33.\n",
            "5. Remaining seeds after step 4: 200 - 33.33 = 166.67.\n",
            "6. Seeds that sprout and are immediately eaten: 166.67 * 1/2 = 83.33.\n",
            "7. Remaining seeds after step 6: 166.67 - 83.33 = 83.34 (rounded to two decimal places).\n",
            "This calculation assumes the fractional results are rounded, but for an exact answer, 83.33 is the number of surviving seeds.\n",
            "\n",
            "Now, the number of surviving dandelions that can flower is approximately 83.\n",
            "</reasoning>\n",
            "<answer>\n",
            "83\n",
            "</answer> \n",
            "Extracted:\n",
            "83\n",
            "-------------------- Question:\n",
            "Macey saves to buy herself a shirt that costs $3. She was able to save $1.50 already. How many weeks does she need to save for the remaining amount if she saves $0.50 per week? \n",
            "Answer:\n",
            "3 \n",
            "Response:\n",
            "<reasoning>\n",
            "Macey needs to save a total of $3 for the shirt. She has already saved $1.50. The remaining amount to be saved is therefore $3 - $1.50 = $1.50. Since she saves $0.50 per week, to find out how many weeks she needs, we divide the remaining amount by her weekly savings: $1.50 / $0.50 per week = 3 weeks. \n",
            "</reasoning>\n",
            "<answer>\n",
            "Macey needs to save for 3 more weeks to buy the shirt.\n",
            "</answer> \n",
            "Extracted:\n",
            "Macey needs to save for 3 more weeks to buy the shirt.\n",
            "-------------------- Question:\n",
            "For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive? \n",
            "Answer:\n",
            "12 \n",
            "Response:\n",
            "<reasoning>\n",
            "To solve this problem, we will calculate the money received from recycling cans and the money received from recycling newspapers separately and then sum them up.\n",
            "\n",
            "For the cans: \n",
            "Given 144 cans, and 12 cans per $0.50, we calculate the number of $0.50 units:\n",
            "\\[ \\text{Number of $0.50 units from cans} = \\frac{144}{12} = 12 \\]\n",
            "The total money received from cans is:\n",
            "\\[ 12 \\times 0.50 = 6.00 \\]\n",
            "\n",
            "For the newspapers:\n",
            "Given 20 kg of newspapers, and 5 kg of newspapers per $1.50, we calculate the number of $1.50 units:\n",
            "\\[ \\text{Number of $1.50 units from newspapers} = \\frac{20}{5} = 4 \\]\n",
            "The total money received from newspapers is:\n",
            "\\[ 4 \\times 1.50 = 6.00 \\]\n",
            "\n",
            "Finally, to find the total amount of money received:\n",
            "\\[ 6.00 + 6.00 = 12.00 \\]\n",
            "</reasoning>\n",
            "<answer>\n",
            "The total amount of money received is $12.00. \n",
            "</answer> \n",
            "Extracted:\n",
            "The total amount of money received is $12.00.\n",
            "Standard GRPO model saved to: /content/drive/MyDrive/grpo_model/standard_grpo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting DGRPO training...\")\n",
        "dgrpo_results = train_dgrpo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3c81f207d4b34b3c8c7b1ee2e954b48d",
            "ed33fcd932cc4ba1b2d6cde15cc7a113",
            "4db0545daf5247f6ac863405b330044d",
            "0f928727ce214519bc3027636f9428db",
            "c7c12c76c26c4c10b5a941c20039eaff",
            "3392a7b6202341cba81339b1fdcbaa9a",
            "2ef9354528ef46ac8fd6a458e6271a81",
            "42ae534f0fb14955a7e0a0e03e2c4163",
            "36bb47d61e904c99b02e7dcd6420803a",
            "1b06626d2c554f108a8bcc59349cbfc3",
            "e8f971b714994c508187eec1283d836a",
            "eb3abf40ab484750a0bb0a77a8aed751",
            "de9673337c9141418bd0c9a134a0a96d",
            "0df12c626f87453e8895ab6da8b79896",
            "de9cbdfd43dc4b66beeb99c494bff989",
            "47363e8332474b44a123a60b406d6970",
            "b18acf8f658b402181f799e5186de98f",
            "907eb685a9ec4a99be3c4db850f1206d",
            "3b9801e263e542c1b552e0863114408b",
            "6711610bb76848e49200be1a107d7362",
            "d9dc5d3c87f4418893b05e4ded83dd41",
            "4b7c5cc37ee549468f61c18ee05402b2"
          ]
        },
        "id": "c9ziSGptS0a8",
        "outputId": "3491e127-5880-48c6-ab79-b2a89c37ac7b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DGRPO training...\n",
            "Creating model for DGRPO...\n",
            "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.8.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 13.82%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
            "Unsloth: vLLM's KV Cache can use up to 3.05 GB. Also swap space = 6 GB.\n",
            "INFO 03-23 23:37:06 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 03-23 23:37:06 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
            "INFO 03-23 23:37:08 [model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 03-23 23:37:08 [loader.py:1137] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 03-23 23:37:09 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
            "INFO 03-23 23:37:09 [weight_utils.py:307] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c81f207d4b34b3c8c7b1ee2e954b48d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb3abf40ab484750a0bb0a77a8aed751"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-23 23:37:12 [model_runner.py:1146] Model loading took 2.4314 GB and 3.438418 seconds\n",
            "INFO 03-23 23:37:14 [worker.py:267] Memory profiling takes 1.02 seconds\n",
            "INFO 03-23 23:37:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.14) = 5.47GiB\n",
            "INFO 03-23 23:37:14 [worker.py:267] model weights take 2.43GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.87GiB; the rest of the memory reserved for KV Cache is 2.17GiB.\n",
            "INFO 03-23 23:37:14 [executor_base.py:111] # cuda blocks: 3943, # CPU blocks: 10922\n",
            "INFO 03-23 23:37:14 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 30.80x\n",
            "INFO 03-23 23:37:15 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 23/23 [00:40<00:00,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-23 23:37:55 [model_runner.py:1570] Graph capturing finished in 41 secs, took 0.08 GiB\n",
            "INFO 03-23 23:37:55 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 43.52 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DGRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 50 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
            " \"-____-\"     Trainable parameters = 119,734,272/3,000,000,000 (3.99% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 08:18, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / xmlcount_reward_func</th>\n",
              "      <th>rewards / soft_format_reward_func</th>\n",
              "      <th>rewards / strict_format_reward_func</th>\n",
              "      <th>rewards / int_reward_func</th>\n",
              "      <th>rewards / <lambda></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.779000</td>\n",
              "      <td>0.177374</td>\n",
              "      <td>437.625000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.779000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.313250</td>\n",
              "      <td>0.082907</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.313250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.125250</td>\n",
              "      <td>0.939994</td>\n",
              "      <td>258.375000</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>-0.437750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.002625</td>\n",
              "      <td>0.133084</td>\n",
              "      <td>161.625000</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>-0.002625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.409250</td>\n",
              "      <td>0.241823</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>0.000821</td>\n",
              "      <td>-0.409250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.334531</td>\n",
              "      <td>1.174848</td>\n",
              "      <td>272.375000</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>-0.371000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.518031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.329625</td>\n",
              "      <td>0.099998</td>\n",
              "      <td>241.375000</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>-0.329625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.107125</td>\n",
              "      <td>0.357832</td>\n",
              "      <td>447.000000</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>-0.107125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003750</td>\n",
              "      <td>0.870134</td>\n",
              "      <td>227.625000</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>-0.308750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.030125</td>\n",
              "      <td>0.154925</td>\n",
              "      <td>172.250000</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>-0.030125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.194750</td>\n",
              "      <td>0.106745</td>\n",
              "      <td>204.250000</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>-0.194750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.140870</td>\n",
              "      <td>135.625000</td>\n",
              "      <td>0.001204</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.005067</td>\n",
              "      <td>1.103880</td>\n",
              "      <td>328.750000</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>-0.637625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.507558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.348473</td>\n",
              "      <td>1.165324</td>\n",
              "      <td>206.750000</td>\n",
              "      <td>0.001018</td>\n",
              "      <td>-0.278125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.501598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.225375</td>\n",
              "      <td>0.919510</td>\n",
              "      <td>290.375000</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>-0.537875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.222125</td>\n",
              "      <td>0.272321</td>\n",
              "      <td>184.375000</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>-0.222125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.326500</td>\n",
              "      <td>0.220880</td>\n",
              "      <td>223.375000</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>-0.326500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.565250</td>\n",
              "      <td>0.260951</td>\n",
              "      <td>284.375000</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>-0.565250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.313375</td>\n",
              "      <td>0.164027</td>\n",
              "      <td>241.875000</td>\n",
              "      <td>0.001126</td>\n",
              "      <td>-0.313375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.488250</td>\n",
              "      <td>0.128873</td>\n",
              "      <td>290.875000</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>-0.488250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024625</td>\n",
              "      <td>0.878178</td>\n",
              "      <td>223.375000</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>-0.287875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.326375</td>\n",
              "      <td>0.152816</td>\n",
              "      <td>272.250000</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>-0.326375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.533000</td>\n",
              "      <td>0.405666</td>\n",
              "      <td>343.500000</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>-0.533000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.025125</td>\n",
              "      <td>0.855720</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>0.001518</td>\n",
              "      <td>-0.337625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.130625</td>\n",
              "      <td>0.212364</td>\n",
              "      <td>227.625000</td>\n",
              "      <td>0.000787</td>\n",
              "      <td>-0.130625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.869840</td>\n",
              "      <td>1.273721</td>\n",
              "      <td>212.250000</td>\n",
              "      <td>0.002279</td>\n",
              "      <td>-0.114250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.609090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.605000</td>\n",
              "      <td>0.780709</td>\n",
              "      <td>512.000000</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>-0.605000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.193000</td>\n",
              "      <td>0.198077</td>\n",
              "      <td>212.625000</td>\n",
              "      <td>0.002521</td>\n",
              "      <td>-0.193000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.952164</td>\n",
              "      <td>1.262679</td>\n",
              "      <td>152.125000</td>\n",
              "      <td>0.003557</td>\n",
              "      <td>-0.014750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.779414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.849203</td>\n",
              "      <td>1.218138</td>\n",
              "      <td>132.375000</td>\n",
              "      <td>0.015829</td>\n",
              "      <td>0.122875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.538828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.088000</td>\n",
              "      <td>0.311699</td>\n",
              "      <td>213.750000</td>\n",
              "      <td>0.002675</td>\n",
              "      <td>-0.088000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.280375</td>\n",
              "      <td>0.102902</td>\n",
              "      <td>236.000000</td>\n",
              "      <td>0.002399</td>\n",
              "      <td>-0.280375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.038875</td>\n",
              "      <td>0.913547</td>\n",
              "      <td>282.375000</td>\n",
              "      <td>0.002837</td>\n",
              "      <td>-0.351375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.068625</td>\n",
              "      <td>0.059301</td>\n",
              "      <td>139.625000</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>0.068625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.308750</td>\n",
              "      <td>0.894051</td>\n",
              "      <td>162.125000</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>-0.003750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.348596</td>\n",
              "      <td>1.228418</td>\n",
              "      <td>207.125000</td>\n",
              "      <td>0.009298</td>\n",
              "      <td>-0.321125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.544721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.425125</td>\n",
              "      <td>0.261910</td>\n",
              "      <td>277.375000</td>\n",
              "      <td>0.002359</td>\n",
              "      <td>-0.425125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.064250</td>\n",
              "      <td>0.117359</td>\n",
              "      <td>235.250000</td>\n",
              "      <td>0.004487</td>\n",
              "      <td>-0.064250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.122625</td>\n",
              "      <td>0.217466</td>\n",
              "      <td>237.875000</td>\n",
              "      <td>0.002901</td>\n",
              "      <td>-0.122625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>1.820200</td>\n",
              "      <td>1.234004</td>\n",
              "      <td>168.750000</td>\n",
              "      <td>0.008530</td>\n",
              "      <td>-0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.555200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.074750</td>\n",
              "      <td>1.092915</td>\n",
              "      <td>271.750000</td>\n",
              "      <td>0.002661</td>\n",
              "      <td>-0.237750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.209875</td>\n",
              "      <td>0.155022</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>0.004921</td>\n",
              "      <td>-0.209875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>1.396435</td>\n",
              "      <td>1.317599</td>\n",
              "      <td>201.625000</td>\n",
              "      <td>0.003537</td>\n",
              "      <td>-0.233125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>1.317060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.083375</td>\n",
              "      <td>0.491134</td>\n",
              "      <td>350.875000</td>\n",
              "      <td>0.002689</td>\n",
              "      <td>-0.083375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.471500</td>\n",
              "      <td>0.245733</td>\n",
              "      <td>328.750000</td>\n",
              "      <td>0.002238</td>\n",
              "      <td>-0.471500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.135750</td>\n",
              "      <td>0.113538</td>\n",
              "      <td>215.125000</td>\n",
              "      <td>0.002497</td>\n",
              "      <td>-0.135750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.278367</td>\n",
              "      <td>1.176619</td>\n",
              "      <td>241.500000</td>\n",
              "      <td>0.004190</td>\n",
              "      <td>-0.349000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.502367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.630195</td>\n",
              "      <td>1.412821</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>0.005422</td>\n",
              "      <td>-0.244750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.562445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.067375</td>\n",
              "      <td>0.150065</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.002995</td>\n",
              "      <td>0.067375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.370500</td>\n",
              "      <td>0.361389</td>\n",
              "      <td>325.125000</td>\n",
              "      <td>0.003441</td>\n",
              "      <td>-0.370500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DGRPO model saved to: /content/drive/MyDrive/grpo_model/dgrpo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine results manually\n",
        "comparison_results = {\n",
        "    \"standard\": standard_results,\n",
        "    \"dgrpo\": dgrpo_results\n",
        "}\n",
        "\n",
        "print(\"Plotting comparison metrics...\")\n",
        "plot_comparison(comparison_results)"
      ],
      "metadata": {
        "id": "lFY5jURMKydw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "74115350-1b6e-407e-b6db-964badffe42a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting comparison metrics...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistical Comparison:\n",
            "======================\n",
            "Standard GRPO Final Loss: 0.0001\n",
            "DGRPO Final Loss: 0.0001\n",
            "DGRPO Loss Improvement: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the models and get results\n",
        "test_results = test_models(comparison_results)"
      ],
      "metadata": {
        "id": "gIMZHU4IaeND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save test model (optional)\n",
        "# Uncomment to save the test model\n",
        "# Save the LoRA adapter from test run\n",
        "test_lora_dir = \"/content/drive/MyDrive/grpo_model/test_lora\"\n",
        "os.makedirs(test_lora_dir, exist_ok=True)\n",
        "model.save_lora(test_lora_dir)\n",
        "print(f\"Test LoRA adapter saved to: {test_lora_dir}\")"
      ],
      "metadata": {
        "id": "hRXbDo51K1Kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32faecb3daa949949bef53c6e5a62ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee26cff63af747af934281ac5ffcbc85",
              "IPY_MODEL_0ac2cd5c7e6f49ddac2bb94906f5ca34",
              "IPY_MODEL_0f11f0e1beea415a8ca17c8108eb2112"
            ],
            "layout": "IPY_MODEL_63c3c69b9e534b48b746b0b7f72ccaea"
          }
        },
        "ee26cff63af747af934281ac5ffcbc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b885df2b0994d0581375aa5a9f050d5",
            "placeholder": "​",
            "style": "IPY_MODEL_f23da13a375242379efff25fac78c164",
            "value": ""
          }
        },
        "0ac2cd5c7e6f49ddac2bb94906f5ca34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000eef1efc51416b83c4f0a2adbe703d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9035c8de1e194d50bceeead68cd89708",
            "value": 1
          }
        },
        "0f11f0e1beea415a8ca17c8108eb2112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628a653b5e274cb2938de68e50c5e102",
            "placeholder": "​",
            "style": "IPY_MODEL_06990b32820e4975a6529f10e0c35255",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  1.11it/s]\n"
          }
        },
        "63c3c69b9e534b48b746b0b7f72ccaea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b885df2b0994d0581375aa5a9f050d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23da13a375242379efff25fac78c164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "000eef1efc51416b83c4f0a2adbe703d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9035c8de1e194d50bceeead68cd89708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "628a653b5e274cb2938de68e50c5e102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06990b32820e4975a6529f10e0c35255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbe5bf88dca94691ab8c8457c9c986d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46eae2205f5b427cbd23fdeb702d5d7b",
              "IPY_MODEL_9d37116b2b5b4d53b0a0b700d685783c",
              "IPY_MODEL_609ced53e08e4b51b1d24dffa1b08e69"
            ],
            "layout": "IPY_MODEL_132779d2b05b42c1948d6485aed3be84"
          }
        },
        "46eae2205f5b427cbd23fdeb702d5d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_561acbb114cb4991845f04e5b2dd3e94",
            "placeholder": "​",
            "style": "IPY_MODEL_55851eeb01e745bbbf0113ce2e489361",
            "value": ""
          }
        },
        "9d37116b2b5b4d53b0a0b700d685783c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b59728bb84c045e2b48e9286b078e598",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd10ee1bfe66483abda594687e5998f9",
            "value": 1
          }
        },
        "609ced53e08e4b51b1d24dffa1b08e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae719b74a7aa49e5814fa0e77aee1edc",
            "placeholder": "​",
            "style": "IPY_MODEL_df040e33f55d4a1e907f80405be8b799",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.02s/it]\n"
          }
        },
        "132779d2b05b42c1948d6485aed3be84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "561acbb114cb4991845f04e5b2dd3e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55851eeb01e745bbbf0113ce2e489361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59728bb84c045e2b48e9286b078e598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd10ee1bfe66483abda594687e5998f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae719b74a7aa49e5814fa0e77aee1edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df040e33f55d4a1e907f80405be8b799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66bb45c4a6004f538fd7471e598ef5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd1e8ea656ef4fcf803b726475d5fe1d",
              "IPY_MODEL_6bb91d0044db47e095f8706a12b86928",
              "IPY_MODEL_4a4b07aae69142c586655347dae4b843"
            ],
            "layout": "IPY_MODEL_0c9ea8584cf94ad1a45676bf47e3a158"
          }
        },
        "bd1e8ea656ef4fcf803b726475d5fe1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_323a822274e1424984277d8408be6f10",
            "placeholder": "​",
            "style": "IPY_MODEL_61ae1958579b489d800e8deb70bf9e19",
            "value": ""
          }
        },
        "6bb91d0044db47e095f8706a12b86928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30cb440cea2d4c609bb54b6cad72420a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccba1f9832b64683b95befe97b88c122",
            "value": 1
          }
        },
        "4a4b07aae69142c586655347dae4b843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d20bd32a85e4268a16ca2952dffd067",
            "placeholder": "​",
            "style": "IPY_MODEL_88bb3d3b38d84bf2a1bdd7699d553f90",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  1.10it/s]\n"
          }
        },
        "0c9ea8584cf94ad1a45676bf47e3a158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323a822274e1424984277d8408be6f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ae1958579b489d800e8deb70bf9e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30cb440cea2d4c609bb54b6cad72420a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccba1f9832b64683b95befe97b88c122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d20bd32a85e4268a16ca2952dffd067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88bb3d3b38d84bf2a1bdd7699d553f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d793345c2f049bb94562e27f2838959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2ab9d9f5be24b5397eb24e1d7c6dabe",
              "IPY_MODEL_0861df59a45846668df69ad8452de673",
              "IPY_MODEL_ef157a89ec5c4a8e94012519e46544f4"
            ],
            "layout": "IPY_MODEL_469f4b2420e5445999c81f66ebf0b7f2"
          }
        },
        "f2ab9d9f5be24b5397eb24e1d7c6dabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e49f9ee5d7945359e91551d3d3901ed",
            "placeholder": "​",
            "style": "IPY_MODEL_3b9c5494644c429a8e65ebb076ceae22",
            "value": ""
          }
        },
        "0861df59a45846668df69ad8452de673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_066205d757254a3faecde35f99d43da1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0918513db65f4607aa4342e07079228a",
            "value": 1
          }
        },
        "ef157a89ec5c4a8e94012519e46544f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75b0fb832232492d886f959d54ed3a46",
            "placeholder": "​",
            "style": "IPY_MODEL_a404ad22697a487abaa296dcb458c59e",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.04s/it]\n"
          }
        },
        "469f4b2420e5445999c81f66ebf0b7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e49f9ee5d7945359e91551d3d3901ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9c5494644c429a8e65ebb076ceae22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "066205d757254a3faecde35f99d43da1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0918513db65f4607aa4342e07079228a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75b0fb832232492d886f959d54ed3a46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a404ad22697a487abaa296dcb458c59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c81f207d4b34b3c8c7b1ee2e954b48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed33fcd932cc4ba1b2d6cde15cc7a113",
              "IPY_MODEL_4db0545daf5247f6ac863405b330044d",
              "IPY_MODEL_0f928727ce214519bc3027636f9428db"
            ],
            "layout": "IPY_MODEL_c7c12c76c26c4c10b5a941c20039eaff"
          }
        },
        "ed33fcd932cc4ba1b2d6cde15cc7a113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3392a7b6202341cba81339b1fdcbaa9a",
            "placeholder": "​",
            "style": "IPY_MODEL_2ef9354528ef46ac8fd6a458e6271a81",
            "value": ""
          }
        },
        "4db0545daf5247f6ac863405b330044d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ae534f0fb14955a7e0a0e03e2c4163",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36bb47d61e904c99b02e7dcd6420803a",
            "value": 1
          }
        },
        "0f928727ce214519bc3027636f9428db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b06626d2c554f108a8bcc59349cbfc3",
            "placeholder": "​",
            "style": "IPY_MODEL_e8f971b714994c508187eec1283d836a",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  1.10it/s]\n"
          }
        },
        "c7c12c76c26c4c10b5a941c20039eaff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3392a7b6202341cba81339b1fdcbaa9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ef9354528ef46ac8fd6a458e6271a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42ae534f0fb14955a7e0a0e03e2c4163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36bb47d61e904c99b02e7dcd6420803a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b06626d2c554f108a8bcc59349cbfc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8f971b714994c508187eec1283d836a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb3abf40ab484750a0bb0a77a8aed751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de9673337c9141418bd0c9a134a0a96d",
              "IPY_MODEL_0df12c626f87453e8895ab6da8b79896",
              "IPY_MODEL_de9cbdfd43dc4b66beeb99c494bff989"
            ],
            "layout": "IPY_MODEL_47363e8332474b44a123a60b406d6970"
          }
        },
        "de9673337c9141418bd0c9a134a0a96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18acf8f658b402181f799e5186de98f",
            "placeholder": "​",
            "style": "IPY_MODEL_907eb685a9ec4a99be3c4db850f1206d",
            "value": ""
          }
        },
        "0df12c626f87453e8895ab6da8b79896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b9801e263e542c1b552e0863114408b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6711610bb76848e49200be1a107d7362",
            "value": 1
          }
        },
        "de9cbdfd43dc4b66beeb99c494bff989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9dc5d3c87f4418893b05e4ded83dd41",
            "placeholder": "​",
            "style": "IPY_MODEL_4b7c5cc37ee549468f61c18ee05402b2",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.08s/it]\n"
          }
        },
        "47363e8332474b44a123a60b406d6970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b18acf8f658b402181f799e5186de98f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "907eb685a9ec4a99be3c4db850f1206d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b9801e263e542c1b552e0863114408b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6711610bb76848e49200be1a107d7362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9dc5d3c87f4418893b05e4ded83dd41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b7c5cc37ee549468f61c18ee05402b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}